---
title: "Manuscript"
format: docx
editor: 
  markdown: 
    wrap: 72
---

```{r, include = FALSE}
# library loading and installing as necessary


# relevant libraries required for this script
packages <- c("ggplot2", "here", "metafor", "gridExtra", "knitr", "magrittr", "dplyr")

# check, whether library already installed or not - install and load as needed:
apply(as.matrix(packages), MARGIN = 1, FUN = function(x) {
  
  pkg_avail <- nzchar(system.file(package = x))   # check if library is installed on system
  
  if(pkg_avail){
    require(x, character.only = TRUE)             # load the library, if already installed
    
  }else{
    install.packages(x, repos = "https://ftp.gwdg.de/pub/misc/cran/")                           # install the library, if missing
    require(x, character.only = TRUE)             # load after installation
  }
})

source(here("ReLiability_Function-library.R"))

ES_rma_df <- read.csv(here("Data/Processed/Aggregates_ES_analysis.csv"))

agg_L <- readRDS(here("Data/Processed/Aggregates_simple.csv"))

MASC_names <- unique(ES_rma_df$MASC)

```



# Score ReLiability – Can we explain effect size heterogeneity using score reliability?

Large-scale collaborative replication efforts have sparked discussions
surrounding the replicability of psychological phenomena. However, for
attempts to estimate a phenomenon’s replicability or to predict whether
a future replication will replicate successfully, heterogeneity is a
crucial parameter that needs to be assessed. In the meta-analytic
context, heterogeneity describes the variation in effect sizes, free of
sampling error, if these are not stable or constant across replications.
Therefore, the presence of heterogeneity implies that some replications
of a single phenomenon may be successful, while others are not. If
heterogeneity grows, meaning the phenomenon’s effect size varies more
strongly for unexplained reasons, the probability of observing an effect
size around zero or even in the negative space grows larger as well. If
we know the mean size and heterogeneity of a phenomenon’s effect size,
we would theoretically be able to establish a baseline of expected
replication rate (reference).

Similarly, it has been argued that heterogeneity in effect sizes is an
indicator of the theory’s “completeness” surrounding the phenomenon.
Linden and Hönekopp argue that “low (as opposed to high) heterogeneity
reflects a more advanced understanding of the subject matter being
studied” (2021, p.2). Similarly, Schuetze and von Hippel (2023) argue
that heterogeneity in effects is an indicator of a vague, poorly
specified theory.

Re-analyses of multi-site direct replications have demonstrated that, if
a psychological effect size can be distinguished from zero,
heterogeneity is present across most phenomena (ref). Large-scale
attempts of direct replications, using identical protocols, such as the
Many Labs studies or Registered Replication Reports (references), for
the first time allow researchers to estimate heterogeneity undistorted
by differences in experimental designs. In re-analyses of these studies,
Olsson-Collentine et al. (2020) identify a positive correlation between
a phenomenon’s effect size and its heterogeneity. Similarly, van Erp et
al. (2017) or Stanley et al. (2018) estimate strong degrees of
heterogeneity across conceptual replications in Psychology. In a
separate re-analysis of large scale direct replications, Renkewitz et
al. (2025) identify substantial heterogeneity in almost all projects
where a non-zero effect could be identified. This aligns with the
correlation found by Olsson-Collentine et al. (2020).

## Effect sizes and score reliability in meta-analysis

In both the initial reports of large scale replication attempts, as well
as the re-analyses by Olsson-Collentine et al. (2020), standardized
effect sizes (from here-on abbreviated as ES) such as Cohen’s d or
Hedge’s g were used. For the remainder of the article, Cohen’s d,
defined in @eq-d, will be used as an exemplary estimate of ES, as it is
used across a wide range of contexts and well understood by a broad
audience.

$$d = \frac{MD}{\sigma_X}$$ {#eq-d}

Here, MD refers to the difference between two groups of interest, while
$\sigma_X$ is the total pooled standard deviation. Since $d$ constitutes
an observed effect size it is affected by sampling error. In terms of
heterogeneity, we are not interested in the variance of $d$, but the
variance of the true, underlying ES $\delta$. The standard error of
Cohen’s $d$, which is used to weight individual estimates in a
random-effects meta-analysis (reference) is computed as defined in
@eq-SEd.

$$SE_d=\sqrt{\frac{n_1+n_2}{n_1n_2}+\frac{d^2}{2\left(n_1+n_2\right)}}$$ {#eq-SEd}

As is widely known, score reliability affects ES. In the context of
classical test theory (CTT) score reliability $\rho_{XX’}$ is defined as
the ratio of true to total score variance, as defined in @eq-rel.

$$\rho_{XX’} = \frac{\sigma^2_T}{\sigma^2_X}=\frac{\sigma^2_T}{\sigma^2_T + \sigma^2_E}$$ {#eq-rel}

In this equation $\sigma_T^2$ refers to the true score variance in the
sense of CTT, meaning the actual variance of the variable, undistorted
by random measurement error. In the same sense, $\sigma_X^2$ refers to
the total variance of scores, including both the true variance and the
random error variance $\sigma_E^2$. This implies that, if true score
variance is assumed to be constant across replications, a lower score
reliability can only occur due to a larger error score variance. Total
score variance would also be larger, therein leading to a smaller ES, as
opposed to a similar data-set where a higher score reliability is
achieved through a smaller error score variance. However, score
reliability is an aspect of a measuring instrument applied to a
population. In the meta-analytic context, as studies tend to be
replicated in different populations, neither true nor error score
variance can be expected to always be identical across replications.
This most likely leads to heterogeneity in score reliability, which, as
discussed, is bound to affect ES heterogeneity as well.

Previous discussions of heterogeneity in score reliability have
exclusively discussed it as a parameter that inflates heterogeneity in
ES, implying that, if score reliability was perfect across all
replications, the actual heterogeneity would have been lower. In their
discussion, Wiernik and Dahlke claim that „*Measurement error variance
will impact the results of meta-analyses in three ways: by (a) biasing
the mean effect size toward zero, (b) inflating effect-size
heterogeneity and confounding moderator effects, and (c) confounding
publication-bias and sensitivity analyses*” (p. 3, 2020). Additionally,
more clearly, they state that “*If the studies included in a
meta-analysis differ in their measures’ reliabilities, heterogeneity
estimates will be artifactually inflated, erroneously suggesting larger
potential moderator effects*” (p. 4, Wiernik & Dahlke, 2020). They base
their claims largely on work done by Hunter and Schmidt (2014), who
claim that “*Variation in reliability across studies causes variation in
the observed effect sizes above and beyond that produced by sampling
error.*” (p. 302). Overall, both references imply that differences in
score (un)reliability inevitably lead to an inflated heterogeneity in
ES.

As both sources Hunter and Schmidt (2014) and Wiernik and Dahlke (2020)
will be referred to repeatedly, we abbreviate these references as H&S
and W&D respectively.

### Attenuation correction

If information concerning score reliability is available, it is possible
to correct the individual ES for its unreliability. This process is also
known as attenuation correction and, while not without its criticisms
(reference), is a widespread practice in Psychology (reference). @eq-dc
describes a simple attenuation correction procedure.

$$d_c = \frac{d}{\sqrt{\hat{\rho}_{xx’}}}$$ {#eq-dc}

Here, $d_c$ describes the attenuation-corrected estimate of $d$, which
is corrected by dividing it by the estimate of score reliability
$\hat{\rho}_{xx’}$. As described in W&D or Lord & Novick (references),
this is essentially the same, as if the corrected estimate of $d_c$ was
constructed using an estimate of true score standard deviation, as in
@eq-dct.

$$d_c = \frac{MD}{\hat{\sigma}_{T}}=\frac{MD}{\sqrt{\hat{\rho}_{xx’} \hat{\sigma}_X^2}}$$ {#eq-dct}

$\hat{\sigma}_T$ can be estimated by simply rearranging the terms from
@eq-rel. Thereby, an estimate of ES is generated, under the premise that
score reliability would have been perfect ($\rho_{xx'} = 1$). As the
corrected estimate of Cohen’s $d$ is larger and requires an additional
unknown parameter, an estimate of score reliability, the larger
uncertainty in this parameter should be reflected in a larger standard
error. The standard error for $d_c$ is defined in @eq-SEdc.

$$SE_{d_c} = \sqrt{SE^2_d \frac{d_c}{d}}$$ {#eq-SEdc}

The claims made by W&D imply that corrections of individual observed ESs
for their unreliability should lead to lower estimates of heterogeneity
in meta-analyses of corrected ES. If heterogeneity in score reliability
adds to ES heterogeneity, attenuation correction procedures should
eliminate that additional variation, as all corrected ES come with
identical (perfect) score reliability.

Re-analyses of the collaborative multi-site replication efforts have
uncovered differences in score reliability across replications of the
same phenomena (ref McShaw etc.). The claims made in W&D and H&S imply
that the ES heterogeneity uncovered in these projects therefore could
potentially be explained by these differences in score reliability. The
same argument has been proposed by both original authors of phenomena
that did not replicate or carry substantial heterogeneity (references)
as well as the authors of the multi-site replication attempts (refs).
Over the following pages, we assess for the first time, whether the
differences in score reliability do indeed explain, and thereby reduce,
the ES heterogeneity observed across the different multi-site
replication efforts. Even though only a limited number of phenomena
allow for such an analysis, we find no evidence for a reduction in ES
heterogeneity after taking score reliability into account. Subsequently,
we discuss why the expectation formulated in H&S does not allow for such
claims. Alternatively, we explore to what extent defining ES as a random
ratio variable helps understand how ES heterogeneity is affected by
differences in score reliability. We close with a discussion on what
future meta-analysts can expect, when dealing with issues of
(un)reliability in assessments of meta-analyses of ES.

## Re-analysis of archival data

We have attempted to collect all openly available multi-site direct
replications on psychological phenomena (Fünderich et al. 2024). The
data-sets of about 50 phenomena , largely stemming from the efforts of
the Many Labs studies, Registered Replication Reports or the
Psychological Science Accelerator, are made openly available in a
standardised format at \[osf.io\], the DRIPHT repository.

### Data

The ManyLabs projects were collaborative efforts to replicate several
psychological phenomena across different research sites, employing
identical protocols. From five published projects, the data for the
first three and the fifth Many Labs projects was made publicly available
when the DRIPHT repository was set up (references). The Registered
Replication Reports are similar collaborative efforts, with the sole
distinction that for each report a single phenomenon was replicated
several times. The Registered Reports 3-10 were added to the DRIPHT
repository, as they employed experimental designs with at least two
groups. Lastly, some projects from the Psychological Science Accelerator
were added to the DRIPHT repository. Different to Many Labs or
Registered Replication Projects, these collaborative efforts, while also
distributed across the globe, do not formally attempt to perform direct
replications, but focus on original research or conceptual replications.
However, as for each project, across all sites the same protocol is
used, and the subjects are distributed across different countries, a
data structure similar to that of a multi-site replication study
emerges. What makes all these collaborative efforts valuable for this
manuscript is the fact that all phenomena collected in the
DRIPHT-repository are making use of two-group designs and are therefore
easily assessed using standardised effect sizes.

While replication data on a decent number of psychological phenomena is
available in the DRIPHT-repository, to demonstrate how incorporating
score reliability affects heterogeneity in ES, selected phenomena need
to fulfil three conditions: (1) As we focus on the use of ES $d$, the
phenomenon needs to be assessed using a two-group design. This is the
case for all projects in the DRIPHT repository; (2) Score reliability
estimates can be derived, this implies that the phenomenon needs to be
measured using either several indicators forming a single scale or by
repeatedly measuring across several timepoints; (3) Score reliability
can only attenuate effect sizes that are not zero in the first place.
Therefore, we focus on phenomena where meta-analytic mean ES can be
statistically distinguished from zero.

As all phenomena in the collection fulfil the first condition, 50
phenomena have been catalogued where the effect is studied by comparing
the outcome across two separate groups. While the majority of designs
make use of some form of control-treatment manipulation, randomly
assigning participants to a condition (20 phenomena), some phenomena
discuss the effect of pre-existing differences, e.g. biological sex (2
phenomena). From the collection of 50 phenomena, 22 fulfil the second
condition, so that estimates of score reliability can be derived. For
these phenomena, the dependent variable used to construct the effect
size is measured by employing more than a single indicator. At the same
time, however, this implies that the remaining 28 phenomena did not make
use of more than a single indicator to measure the dependent variable.
Therefore, for the majority of phenomena it is not possible to assess in
how far measurement quality is sufficiently high in terms of e.g.
internal consistency or low random error variance.

From the 22 phenomena that employed more than a single indicator, 19
made use of Likert-style items, where respondents would indicate their
agreement to some kind of statement. The scale-length varied from 3 to
10-points. The items measuring the remaining 3 phenomena were coded
dichotomously, as responses were right or wrong (2 cases), or expressed
agreement vs disagreement dichotomously (1 case). Subsequently, those
phenomena need to be identified where the meta-analytic mean effect size
is statistically significant different from zero. This will be done as a
first step of this manuscript’s analysis procedure. Detailed information
concerning the different phenomena and how they were measured can be
found at \[osf-link\], while a brief summary can be found in Table 1.

To assess whether ES heterogeneity can be reduced by correcting for
score reliability, estimates of meta-analytic heterogeneity are compared
across two situations: first, heterogeneity of raw ES, as computed in
equation 1 is assessed. Secondly, ES are corrected for imperfect
reliability. Computing the heterogeneity of these corrected ES allows
for an assessment, whether the heterogeneity did indeed shrink compared
to the first situation, as predicted in W&D and H&S.

### Methods

For each replication of a phenomenon, Cohen’s $d$ is computed as an
estimate of raw ES, according to @eq-d, including its standard error
(@eq-SEd). Subsequently, a random-effects meta-analysis is performed
using metafor version X.XX. To estimate meta-analytic mean ES and
heterogeneity, the REML-estimator is used (ref). To identify which
phenomena pass criterion (3), using a Wald-type significance test we
assess for which phenomenon the meta-analytic mean ES is significantly
different from zero. For this hypothesis test, as all other hypothesis
tests in this manuscript, a significance level of .05 is used.

Additionally, estimates of score reliability are derived. While not
without criticism (references), Cronbach’s Alpha is used to estimate
score reliability. It is often noted that tau-equivalence is an
unrealistic assumption to hold against real-world data, implying that
other estimates of score reliability, such as McDonald’s Omega
(reference), Guttman’s Lambda 2 or 4 (reference), or the Greatest Lower
Bound (reference) might be better suited. However, all scales employed
in this project are scored by computing the simple mean or sum of
responses across items for each individual. Computing the mean or sum of
several items implicitly assumes tau-equivalence, as each item
contributes equally to the individual’s test score. Therefore,
Cronbach’s Alpha is the better choice to estimate score reliability, as
it avoids a mismatch in assumptions between how the score is computed
and how score reliability is estimated. Using these estimates of
Cronbach’s Alpha, the individual estimates of Cohen’s d are corrected,
according to @eq-dc, including the corrected standard errors (@eq-SEdc).
Thereby, estimates of ES are computed, which are corrected for imperfect
reliability. Again, a random-effects meta-analysis is run using metafor,
generating an estimate of heterogeneity in corrected ES. Different
indicators of heterogeneity are readily available, such as $I^2$, $H^2$
or the coefficient of variation. However, the descriptions found in H&S
and W&D discuss the absolute amount of heterogeneity in terms of
variance ($\tau^2$) or standard deviation ($\tau$). These parameters
discuss the variability of the standardized ES in the population in
terms of variance or standard deviation. Therefore, we also make use of
absolute heterogeneity here.

Subsequently, in order to assess whether ES heterogeneity was indeed
reduced by the attenuation correction procedure, the estimates of
heterogeneity in uncorrected ES and corrected ES are compared. Using
Cochran’s Q-test, we assess whether the estimates of heterogeneity are
statistically significantly different from 0. Only for those projects
where we have sufficient confidence that the individual estimates of
heterogeneity are larger than zero can we actually begin to interpret
whether there was any change.

To identify whether a reduction in ES heterogeneity is indeed
accompanied by differences in score reliability, the estimates of score
reliability are assessed meta-analytically as well. To do so, a
Reliability Generalization Meta-Analysis is performed (references). This
entails that individual estimates of score reliability are adequately
transformed using Bonett’s transformation:
$T_{\hat{\rho}} = ln(1 - \hat{\rho})$. This transformation has
variance-stabilising properties and therefore allows for adequate
inferences concerning heterogeneity in score reliability (references).
Cochran’s Q-test is used to identify statistically significant
heterogeneity in transformed score reliability. Estimates derived from a
Reliability Generalization Meta-Analysis using these transformations can
be back-transformed to the original score reliability scale
(references):

$$\mu_{\rho{xx'}} \approx 1 - exp(\mu_{T\left[\rho_{XX^\prime}\right]})-\frac{1}{2} exp(\mu_{T\left[\rho_{xx'}\right]})\tau_{T\left[\rho_{XX^\prime}\right]}^2$$ {#eq-rel_back_mu}

$$\tau_{\rho_{xx'}}^2 \approx exp(\mu_{T\left[\rho_{xx'}\right]})2 \tau_{T\left[\rho_{xx'}\right]}^2+exp(\mu_{T\left[\rho_{xx'}\right]})^2\tau_{T\left[\rho_{xx'}\right]}^4$$ {#eq-rel_back_tau}

These procedures are followed separately for each phenomenon. The
statistical programming language R, Version X.XXX (reference) is used
for all data manipulation and statistical analysis.

### Results

[Equations @eq-d], [-@eq-SEd], [-@eq-dc], and [-@eq-SEdc] were used to
generate estimates of standardized ES, both uncorrected and corrected,
with their corresponding standard errors. Generally, this leads to
larger (absolute) ES and larger standard errors. As an example from the
12 selected phenomena, the results from this procedure on Nosek’s
Explicit Art sex differences phenomenon are displayed in the forest plot
in Figure 1. Here, grey dots represent the uncorrected ES for each
sample, while black dots represent the corrected ES in each sample. The
bars surrounding these dots show the respective 95%-Cis.

In @fig-forest it becomes apparent that the reliability attenuation
procedure leads to an increase in the individual absolute effect size,
as the black dots are moved further away from zero. Simultaneously, the
standard errors grew larger after the attenuation correction, which
leads to larger 95%-CIs. This is most easily observed for the
mturk-sample, which already had a rather large confidence interval to
begin with. Additionally, in the diamond at the bottom of the figure, we
can see that the meta-analytic estimate mean ES is also larger after the
attenuation correction took place. In @tbl-meanES, the estimates and
tests on the meta-analytic average ES, both corrected and uncorrected,
can be found.



```{r, echo = FALSE}
#| label: fig-forest
#| fig-cap: Forest Plot Nosek_Explicit_Art
forest_plot_rel(ES_rma_df[which(ES_rma_df$MASC == "Nosek_Explicit_Art"),], agg_L[[which(MASC_names == "Nosek_Explicit_Art")]])
```

```{r, echo = FALSE}
#| label: tbl-meanES
#| tbl-cap: Meta-analytic average  ES
knitr::kable(x = read.csv(here("Tables/Mean_ES.csv")))
```



@tbl-meanES demonstrates that what we observed in Figure 1 holds across
all 22 phenomena. All meta-analytic effect sizes are larger after
applying an attenuation correction procedure. Similarly, the uncertainty
quantified in the estimates’ standard error is larger. Additionally,
@tbl-meanES highlights which phenomena pass criterion (3) – the effect
size must be statistically significantly distinguishable from zero. 12
phenomena pass this criterion, highlighted in boldface. It may be
remarked that the attenuation correction procedure, in this data,
appears to have no influence on whether a phenomenon passes the
criterion. The differences in p-value are rather small and lead to no
difference in conclusion, regarding a significance level of .05.



```{r, echo = FALSE}
#| label: tbl-tauES
#| tbl-cap: Tests for ES Heterogeneity
knitr::kable(x = read.csv(here("Tables/Heterogeneity_ES.csv")))
```



Cases incompatible with claims made in W&D and H&S are highlighted using
boldface.

However, what @fig-forest and @tbl-meanES can not inform us about is in
how far the heterogeneity has changed after correcting for
(un)reliability. In @tbl-tauES, the estimates and tests of heterogeneity
concerning uncorrected and corrected ES on the 12 remaining phenomena,
which passed all criteria (1) – (3), can be found. In the table, the
estimate of heterogeneity $\tau$, with the accompanying QE-test
statistic, degrees of freedom and its p-value are reported, separately
for uncorrected and corrected ES.

Most importantly, @tbl-tauES demonstrates that of 12 phenomena assessed,
for seven phenomena we find patterns incompatible with claims made in
W&D and H&S. For those seven phenomena, the reliability attenuation
correction led to an increase in absolute ES heterogeneity $\tau$. For
the remaining five phenomena, estimates of ES heterogeneity were zero
before and after the correction procedure, leading to no change at all.
This leaves three phenomena where the attenuation correction did indeed
lead to a reduction of heterogeneity. However, it is crucial to note
that the hypothesis tests identified statistically significant
heterogeneity in only four out of 12 phenomena and for none of those a
reduction in heterogeneity took place. In all four cases of
statistically significant heterogeneity, the attenuation correction
procedure led to an increase in heterogeneity, contrary to the W&D and
H&S predictions.

To make sure that actual differences in score reliability exist,
heterogeneity in score reliability is assessed, making use of
Reliability Generalization Meta-Analysis (reference). @tbl-RGMA
summarises the results across all 12 phenomena. Overall, scales for
almost all phenomena appear have produced scores with an average score
reliability larger than .7. Only two phenomena (Albarracin_Priming_SAT
and Alter_Analytic_Processing) come with lower score reliability.
However, for those phenomena we could not identify statistically
significant heterogeneity in effect sizes in @tbl-meanES in the first
place. From the phenomena where the ES heterogeneity grew larger as a
result from the attenuation correction procedure, for six out of seven
phenomena, we observe statistically significant heterogeneity in score
reliability. Only the Shnabel_Willingness_Reconcile_Rev project produced
scores where no heterogeneity in score reliability was identified.
However, for this project, it is crucial to note that only a small
number of replications (8), was available for analysis. Since the power
of Cochran’s Q-test for heterogeneity largely depends on the number of
replications, this result is inconclusive.

Despite this one inconclusive result, for the majority of phenomena
where ES heterogeneity is larger after applying an attenuation
correction, a lack of differences in score reliability can not be made
responsible for results contrary to claims made in W&D and H&S. All
seven phenomena came with an average score reliability from .74 to .9
and for six out of seven phenomena we identified statistically
significant heterogeneity in score reliability.



```{r, echo = FALSE}
#| label: tbl-RGMA
#| tbl-cap: Reliability Generalization Meta-Analysis
knitr::kable(x = read.csv(here("Tables/Results_RMA_alpha.csv")))
```



## Alternative discussion of how score reliability affects ES heterogeneity

In this section, we explore some facets on why we disagree with claims
stated in W&D and H&S. Additionally, we explore some alternative
explanations that might help understand why correcting ES for score
reliability does not lead to an inevitable reduction in heterogeneity.

### Misinterpreted equation in Hunter & Schmidt (XXXX)

Hunter & Schmidt claim to have derived an equation which demonstrates
how heterogeneity in score reliability inflates heterogeneity in ESs.
“If the level of reliability is independent of the true effect size
across studies, then, to a close approximation:” (p. 309)

$$\tau^2_{\delta_0} = [\mu_{\rho_{xx‘}}]^2 \tau^2_{\delta} + [\mu_{\delta}]^2 \tau^2_{\rho_{xx‘}}$$ {#eq-HS}

We adjusted @eq-HS using the notation used throughout this text. Here,
$\delta_0$ refers to the ES, undistorted by sampling error but not
corrected for measuring error, $\delta$ refers to the ES, undistorted by
sampling error and measurement error and ρxx‘ refers to score
reliability.

@eq-HS implies that heterogeneity in $\delta_0$ is essentially a
function where heterogeneity and mean value of $\delta$ are weighted by
mean score reliability $\rho_{xx‘}$ and its heterogeneity. While it may
not be self-evident in the equation itself, it is easy to construct
cases where heterogeneity in $\delta_0$ is inflated or deflated by
heterogeneity in $\rho_{xx‘}$, contradicting Hunter & Schmidt’s
interpretation of this equation. In @fig-hs, the results of employing
@eq-HS to compute the heterogeneity (variance) in corrected ES is
highlighted. All else held equal, only the individual level of mean
reliability $\mu_{\rho_{xx‘}}$ is varied from .5 to .8. Mean corrected
ES $\mu_{\delta}$ is held constant at 1, its variance $\tau^2_{\delta}$
held constant at .04 and the variance of score reliability
$\tau^2_{\rho_{xx‘}}$ held constant at .02.



```{r, echo = FALSE}
#| label: fig-HS
#| fig-cap: Implications by Hunter & Schmidt's @eg-HS

mu_delta <- 1
var_delta <- .2^2
mu_alpha <- seq(from = .5, to = .8, length.out = 100)
var_alpha <- .14^2


var_delta0 <- mu_alpha^2 * var_delta + mu_delta^2 * var_alpha

ggplot() +
  geom_hline(yintercept = .04, colour = "red", linewidth = 1, linetype = "dashed") +
  geom_line(aes(y = var_delta0, x = mu_alpha), linewidth = 2) +
  theme(panel.background = element_rect(fill = "transparent"), 
        plot.background = element_rect(fill = "transparent", colour = "transparent"), 
        panel.grid.major.y = element_line(colour = "grey"),
        panel.grid.major.x = element_line(colour = "grey"),
        panel.grid.minor = element_line(colour = "transparent"),
        axis.ticks = element_line(colour = "grey"),
        strip.background = element_rect(fill = "transparent")) +
  labs(y = expression(tau^2~"["~delta[0]~"]"),
       x = expression(mu~"["~rho[xx]~"]"))
```



In @fig-hs, the red line corresponds to the variance of corrected ES.

If everything else is held constant, the variance in uncorrected ES can
be a function only of the mean level of score reliability
$\mu_{\rho_{xx‘}}$, according to H&S equation. In that case, Figure 2
demonstrates that correcting for imperfect score reliability does not
inevitably lead to lower heterogeneity, as the line representing
heterogeneity in uncorrected ES does cross the red line, representing
the heterogeneity in corrected ES. Depending on the mean level of score
reliability, heterogeneity in uncorrected ES can be higher or lower than
heterogeneity in corrected ES.

However, @eq-hs is additionally flawed, as it violates a crucial
definition. As Hunter & Schmidt point out, @eq-hs is only valid “If the
level of reliability is independent of the true effect size across
studies \[…\]” (2014, p. 309). However, both parameters, ES $\delta$ and
score reliability ρxx‘, can not be independent parameters by definition.
ES $\delta$ is the “true” standardized mean difference, unaffected by
sampling or measurement error. The ES $\delta_0$ is standardized using
the total standard deviation, which is inflated by measurement error.
Equation 1, demonstrating how Cohen’s d is computed demonstrates how the
mean difference is standardized in this case. As demonstrated further
above in @eg-dct, ES $\delta$ is standardized using the true standard
deviation in the sense of CTT. The fact that the true standard deviation
parameter $\sigma_T$ is found both in @eq-dct and @eq-rel demonstrates
that ES $\delta$ and score reliability $\rho_{xx‘}$ can not be
independent variables. Therefore, the basic premise of @eq-hs is not
fulfilled, as the assumption of variable independence could not be
fulfilled.

### Describing ES as a random ratio variable

However, realising that an ES, as defined in @eq-d, is actually a ratio,
allows for a different analytical description of how differences in
score reliability can affect ES heterogeneity. If the phenomenon is
heterogeneous and score reliability varies across replications, it seems
sensible to assume that both numerator and denominator (MD and
$\sigma_X$) from @eq-d vary across replications. In that case, an ES can
be described as a random variable stemming from a ratio distribution. A
ratio distribution is a probability distribution constructed by dividing
one random variable by a second random variable: $Z = \frac{X}{Y}$
(reference). If the distribution of the components is known, first order
taylor approximation may be used to generate estimates of mean and
variance of the ratio variable (reference). Here we use the
Taylor-estimator of the ratio’s variance to demonstrate how differences
in score reliability may affect heterogeneity in ES.

$$\tau^2[Z] \approx \frac{\tau^2[X]}{\mu[Y]^2} - \frac{2\mu[X]}{\mu[Y]^3} cov[X,Y] + \frac{\mu[X]^2}{\mu[Y]^4} \tau^2[Y]$$ {#eq-XYZ_cov}

Assuming that the random variables are uncorrelated, simplifies the
equation to

$$\tau^2[Z] \approx \frac{\tau^2[X]}{\mu[Y]^2} + \frac{\mu[X]^2}{\mu[Y]^4} \tau^2[Y]$$ {#eq-XYZ}

Substituting for ES, unstandardized mean difference MD and pooled
standard deviation $\sigma_X$ leads to

$$\tau^2[\delta_0] \approx \frac{\tau^2[MD]}{\mu[\sigma_x]^2} + \frac{\mu[MD]^2}{\mu[\sigma_x]^4} var[SD]$$ {#eq-ratio_d0}

As stated in @eq-dct, correcting ES for score reliability can be
described through a correction of the pooled standard deviation:
$\sigma_T = \sqrt{\rho_{xx’} \sigma^2_x}$. Since $\rho_{xx'}$ is always
between zero and one, in the case of imperfect score reliability,
$\sigma_T$ will always be smaller than total $\sigma_x$. Additionally,
if the heterogeneity in $\sigma_X$ was introduced purely by differences
in measurement precision – the error score variance $\sigma_E^2$, this
would be removed by the attenuation correction. However, alternatively,
the underlying latent variable we attempted to measure may not be
identically distributed across replications. In that case, true score
variance $\sigma^2_T$ would vary across replications, leading to
differences in both score reliability, corrected ES and raw ES.
Importantly, this implies that even if measurements of perfect score
reliability were taken across the different replications, heterogeneity
in ES would still persist.

In that case, some heterogeneity in $\sigma_T$ would remain, albeit
heterogeneity found in $\sigma_x$. Equation X essentially demonstrates
how mean and heterogeneity of MD, paired with mean and heterogeneity of
total score standard deviation $\sigma_x$ $\sigma_x$ can be used to
estimate heterogeneity in raw ES $\delta_0$. Similarly, we can adjust
Equation X to describe how, instead of total score standard deviation,
mean and variance in true score standard deviation $\sigma_T$ \sigma\_T
affect heterogeneity in corrected ES $\delta$.

$$\tau^2[\delta] \approx \frac{\tau^2[MD]}{\mu[\sigma_T]^2} + \frac{\mu[MD]^2}{\mu[\sigma_T]^4} \tau^2[\sigma_T]$$ {#eq-ratio_d}

The differences between the two [Equations @eq-ratio_d0] and
[-@eq-ratio_d] demonstrates how ES heterogeneity is affected by an
attenuation correction procedure. According to the quotes in H&S and
W&D, @eq-ratio_d0 should lead to a larger value of
$\tau^2\left[\delta_0\right]$, compared to the estimate of
$\tau^2\left[\delta\right]$ from @eq-ratio_d. This, however, does not
follow from any equations shown throughout the text.

What is guaranteed is that the expected value of $\sigma_T$ is smaller
than the expected value of $\sigma_X$. Similarly, the variance in
$\sigma_T$ is guaranteed to be smaller than the uncorrected variance in
$\sigma_X$. However, since the expected value of either standard
deviation is placed in the denominator of the function and the variance
of either is placed in the numerator of the function, @eq-hs is not
sufficient to explain how heterogeneity in ES changes due to the
attenuation correction. Instead, we introduce two additional metrics.

$$R_1 = \frac{\mu_{\sigma^2_T}}{\mu_{\sigma^2_X}}$$ {#eq-R1}

$R_1$ describes the relative size of mean true score variance, compared
to the mean observed score variance. This informs us, how much of the
mean total score variance can be attributed to mean true score variance.
Therefore, this metric is equivalent to the average score reliability.
Large values indicate that correcting the individual score variances (or
standard deviations for that matter) using the attenuation correction
procedure should lead to smaller change. Small values indicate the
opposite, large amounts of random error score variance on average lead
to large changes in mean score variance (or mean standard deviation).

$$R_2 = \frac{\tau^2_{\sigma^2_T}}{\tau^2_{\sigma^2_X}}$$ {#eq-R2}

The metric $R_2$ on the other hand describes, in how far the attenuation
correction procedure has successfully reduced heterogeneity in score
variance $\tau^2_{\sigma^2_X}$. A value of 1 indicates that the
heterogeneity in true score variance $\tau^2_{\sigma^2_T}$, the score
variance after the attenuation correction, is essentially just as large
as the heterogeneity initally observed. Smaller values indicate how much
heterogeneity is "left", after applying an attenuation correction,
relative to the inital heterogeneity. This means that a value of .7
indicates that about 70% of heterogeneity in score variance remains,
even after correcting for differences in score reliability. As only 30%
of score variance heterogeneity could be removed, this would imply that
differences in error score variance $\sigma^2_E$ were responsible for
less than a third of the heterogeneity in score variances found. On the
contrary, more than two thirds of heterogeneity could be attributed to
actual differences in how the underlying true scores are distributed
across samples.

In this section, we attempt to understand under which circumstances the
heterogeneity in ES grows larger as the result of an attenuation
correction procedure, contrary to claims made in W&D and H&S. To do so,
we propose the following inequality, involving [Equations @eq-ratio_d0]
and [-@eq-ratio_d].

$$\tau^2_{\delta} > \tau^2_{\delta_0} \quad  \quad \quad \quad \quad 
\frac{\tau_{MD}^2}{\mu_{\sigma_T}^2} + \frac{\mu_{MD}^2}{\mu_{\sigma_T}^4} \tau^2_{\sigma_T} > \frac{\tau_{MD}^2}{\mu_{\sigma_x}^2} + \frac{\mu_{MD}^2}{\mu_{\sigma_x}^4} \tau^2_{\sigma_x}$$ {#eq-ineq_init}

Using the metrics $R_1$ and $R_2$, defined in [Equations @eq-R1] and
[-@eq-R2], we attempt to understand under which circumstances the
inequality defined in @eq-ineq_init holds true. However, while the
metrics make use of mean and heterogeneity of the score variances,
@eq-ineq_init makes use of mean and heterogeneity of the standard
deviations. In order to incorporate the metrics, it is necessary to
reparameterised the equation. Using the delta method to approximate mean
value and heterogeneity of variance from those of the standard
deviations, we know that (ref):
$$\mu_{\sigma^2_X} \approx \mu^2_{\sigma_X} \quad \quad  and \quad \quad 
CV_{\sigma^2} \approx 2 CV_{\sigma}$$ {#eq-delta_method}

While @eq-delta_method only explicitly contains parameters for the
observed score variance/standard deviation, the same can be done using
the true score variance/standard deviation. Using the approximates
defined in @eq-delta_method for observed and true score variance, we
arrive at a new inequality in @eq-ineq_CV.

$$\frac{\tau_{MD}^2}{\mu_{\sigma^2_t}} + \frac{\mu_{MD}^2}{\mu^2_{\sigma_t^2}} \frac{1}{4} CV^2_{\sigma^2_t} \mu_{\sigma^2_t} > \frac{\tau_{MD}^2}{\mu_{\sigma^2_x}} + \frac{\mu_{MD}^2}{\mu^2_{\sigma_x^2}} \frac{1}{4} CV^2_{\sigma^2_x} \mu_{\sigma^2_x}$$ {#eq-ineq_CV}

@eq-ineq_CV describes that the heterogeneity in corrected ES is larger
than the heterogeneity in uncorrected ES, using parameters of the
distributions of mean differences MD, true score variances\*
$\sigma^2_T$ and observed score variances $\sigma^2_X$. By introducing
the metrics $R_1$ and $R_2$ to this inequality, we can begin to
disentangle which circumstances need to be fulfilled for the inequality
to hold. Rearranging the terms in [Equations @eq-R1] and [-@eq-R2], and
subsequently entering these terms into Equation 18 leads to the
following
$$ \frac{\tau_{MD}^2}{R_1 \mu_{\sigma^2_x}} + \frac{\mu_{MD}^2}{R_1^2 \mu^2_{\sigma^2_x}} \frac{1}{4} R^2_2 CV^2_{\sigma^2_X} R_1 \mu_{\sigma^2_x} > \frac{\tau_{MD}^2}{\mu_{\sigma^2_x}} + \frac{\mu_{MD}^2}{\mu^2_{\sigma_x^2}} \frac{1}{4} CV^2_{\sigma^2_x} \mu_{\sigma^2_x}$$ {#eq-ineq_R}

@eq-ineq_R, again, describes the inequality that heterogeneity in
corrected ES is larger than the heterogeneity in uncorrected ES,
incorporating the metrics $R_1$ and $R_2$. This equation alone is not
sufficient to identify the relevant circumstances required for the
inequality to hold. However, rearranging the terms from @eq-ineq_R leads
to @eq-ineq_fin:

$$ \tau_{MD}^2\left(\frac{1}{R_1} - 1\right) + \mu_{MD}^2 \frac{1}{4} CV^2_{\sigma^2_X} \left(\frac{R_2}{R_1^3}  - 1\right) > 0$$ {#eq-ineq_fin}

The circumstances under which the inequality described in Equation 20
holds, are circumstances where the claims made in W&D and H&S are
directly contradicted, as in those cases the ES heterogeneity is larger
after applying an attenuation correction procedure. Concerning the terms
in @eq-ineq_fin, we know that all terms left-hand of the inequality,
outside of the brackets ($\tau^2_{MD}$, $\mu^2_{MD}$ and
$CV^2_{\sigma^2_X}$), are bound to be positive. For the inequality to
hold, we need the left-hand side of the equation to remain positive,
larger than zero. We can distill two scenarios, under which this
equation should hold: (a) one of the terms inside the brackets
($\frac{1}{R_1} - 1$ and $\frac{R^2_2}{R_1}  - 1$) is positive and large
enough, so that the second term not containing that same bracket is
positively dominated by the first term; or (b) both terms inside the
brackets need to be positive.

Generally, we know that both $R_1$ and $R_2$ are bound to be positive,
as both contain different, strictly positive parameters of the
distributions of true and observed score variance. Additionally, we know
that $R_1$ is equivalent to the average score reliability, and therefore
bound between $0\le R_1\le1$. Therefore, the term inside the first
bracket is bound to be positive ($\frac{1}{R_1} – 1$). Similarly, $R_2$,
as the ratio of true and error score variance heterogeneity, is bound
between $0\le R_2\le1$. If $R_1^3$ is smaller than $R_2$, then the term
inside the second bracket turns negative. Generally, this means that the
inequality can only be violated if the reduction in relative
heterogeneity of score variance ($R_2$) is larger than the the average
score reliability to the power of 3.

Concerning the two scenarios, for scenario (a) to hold true, since the
term involving only $R_1$ in the first bracket can only be positive, the
second term involving both metrics would need to be negative. For the
first term to positively dominate the second term, however, we would
need to make additional assumptions about the size of the parameters
outside of the brackets, specifically about $\tau^2_{MD}$ and
$\mu_{MD}$. This is something we would prefer to avoid, as it involves
additional assumptions about the individual raw effect sizes of the
different phenomena. Additionally, it can be said that as long as the
term in the second bracket involving both metrics is positive, scenario
(b) is true by default. This occurs any time $R_1^3 > R_2$. For example,
if the average score reliability is about .8, if $R_2$ is at least .512
or larger, the inequality holds true and ES heterogeneity grows larger
as a consequence of the attenuation correction procedure, contrary to
claims made in W&D and H&S. This means that for the exemplary score
reliability of .8, a reduction in relative heterogeneity in score
variance of 48.8% ($(1 – R_2)100\%$) or more is necessary for scenario
(b) to no longer be true. The smaller the average score reliability, the
smaller the $R_2$-metric may be for the inequality to still hold true.

Keeping this in mind, even if $R_2$ is smaller than .512, it does not
immediately imply that ES heterogeneity is smaller in $\delta$. For
example, it is unlikely that an $R_2 = .5$ leads to a term that is not
positively dominated by the first term involving only $R_1$. However,
where exactly the cut-off is, when ES heterogeneity starts shrinking due
to the attenuation correction procedure, can not be easily defined
without additional assumptions about the distribution of $MD$.

Generally speaking, the attenuation correction leads to a reduction both
in mean value of $\sigma^2_X$ and in its heterogeneity. While a
reduction in the former typically leads to an increase in heterogeneity
of $d_c$, the latter has the potential to compensate for that increase
and thereby reduce that heterogeneity after the attenuation correction.
This reduction in ES heterogeneity can only occur, if the reduction in
score variance heterogeneity is substantially stronger than the
reduction in mean score variance. An extreme version of such a situation
might occur, if the true score variance is identical across all samples.
This would imply that all heterogeneity in score variance identified can
be attributed to differences in how much random noise is found in the
scores across the samples. In such a case, $R_2$ would be zero and a
reduction in ES heterogeneity due to the attenuation correction would be
guaranteed. This also implies that the lower $R_2$ and the larger the
average score reliability $R_1$, the more likely it is that the
attenuation correction actually does lead to a reduction in ES
heterogeneity.

### Re-analysis of archival data

Table 3 showed that for all phenomena, where we identified statistically
significant ES heterogeneity, the heterogeneity was even larger after
the attenuation correction procedure. Subsequently, in Equation 16 we
demonstrate that if the $R_2$ metric is larger than $\mu[\rho_{xx’}]^3$,
ES heterogeneity is bound to be larger after applying said procedure.
Table 5 reports the meta-analytic estimates of observed and true score
variances, including their relative heterogeneity and metrics $R_1$ and
$R_2$ across all 12 phenomena. In Table 5 we see that for ten out of 12
phenomena, the $R_2$ metric is larger than 1. This implies that for all
measurements concerning these phenomena, the relative heterogeneity in
observed score variances could not be substantially explained by
differences in measurement quality. Correcting for heterogeneity in
error score variance has in fact only increased the extent of relative
heterogeneity present in the score variances. As demonstrated in
Equation 20, this implies that the ES heterogeneity grows even larger,
as we correct for differences in score reliability.



```{r, echo = FALSE}
#| label: tbl-R1R2
#| tbl-cap: Re-analysis concerning metrics $R_1$ and $R_2$
knitr::kable(read.csv(here("Tables/Variances_analysis.csv")))
```



In @tbl-tauES we also found some phenomena where the heterogeneity did
in fact shrink after applying the attenuation correction procedure. This
is not reflected by what we observe in @tbl-R1R2, as $R_2 > R_1$ for
almost all phenomena, which would imply that across all phenomena we
expected ES heterogeneity to grow as a consequence of the attenuation
correction procedure. However, it is crucial to realise that we did not
find statistically significant heterogeneity for any of the phenomena
where ES heterogeneity was reduced. This implies, that we actually could
not statistically distinguish the ES heterogeneity from zero. Therefore,
what seems to be a reduction in ES heterogeneity was most likely caused
by estimation issues, as we could not accurately estimate the extent of
ES heterogeneity, either before or after the attenuation correction.

Lastly, for two out of the 12 phenomena, we find a $CV_{\sigma^2_X}$ of
0, as the estimate of absolute heterogeneity $\tau_{\sigma^2_X}$ was 0
as well. For these phenomena, it is therefore also impossible to compute
the $R_2$ metric. While for the Alter_Analytic_Processing phenomenon we
did not identify statistically significant ES heterogeneity in the first
place, we did find some for the Shnabel_Willingness_Reconcile_Rev
phenomenon. At the same time, we did not identify statistically
significant heterogeneity in score reliability. With zero heterogeneity
in score variances, Equation 13 implies that the ES heterogeneity is
bound to grow larger due to the attenuation procedure. Also, this
phenomenon was assessed with a particularly low number of replications
(8). Most likely, adequate power to detect heterogeneity in score
reliability and observed score variance requires a larger number of
replications than adequate power to detect heterogeneity. In that case,
the results from Table 5, concerning the
Shnabel_Willingness_Reconcile_Rev phenomenon might alternatively be
explained by power-issues. Similarly, while Table 3 does not imply
specific expectations concerning the presence of score variance
heterogeneity in the Alter_Analytic_Processing phenomenon, the low
number of replications (20) makes it hard to distinguish whether the
estimate of $\tau_{\sigma^2_X} = 0$ is actually sensible or can also be
attributed to low power.

## Discussion

Across twelve exemplary archival datasets, we demonstrated that claims
made in H&S and W&D do not hold in empirical observations. Even though
removing the impact of score reliability on ES heterogeneity by means of
attenuation correction procedures, they do not lead to a reduction in
heterogeneity as claimed in H&S and W&D. Inferences derived from the
equation supplied in H&S (ref), Equation 5, also do not warrant such a
conclusion. Alternatively, describing standardised effect sizes in terms
of a ratio distribution and approximating its variance, we demonstrated
that differences in score reliability may both inflate or deflate
variance in ES, depending on how the relative heterogeneity in score
variance changes due to the attenuation correction procedure. The metric
$R_2$, compared with the metric $R_1$ or the average score reliability,
informs us on whether relative score variance heterogeneity decreases or
increases with the attenuation correction.

These results fit in with work recently published/preprinted by
Olsson-Collentine et al. (2023). In a large simulation scheme, they
demonstrate that differences in score reliability across administrations
typically deflate heterogeneity in uncorrected correlations. Only as the
true heterogeneity in correlations grows larger, score reliability
differences actually inflate heterogeneity as claimed in W&D. While
correlations and standardized effect sizes are not identical, the way
score (un)reliability affects these parameters is highly similar. Both
correlations and ES are deflated by (un)reliability, implying that an
attenuation correction increases mean values of these parameters, while
the impact on standard deviations used for standardisation purposes
needs to be checked via $R_2$.

The analytical arguments presented indicate that an attenuation
correction procedure, even if no differences between score reliability
exist, lead to larger ES heterogeneity. It appears that, as the
meta-analytical mean ES grows larger due to the correction procedure, so
does the ES heterogeneity, if homogeneity is present. If heterogeneity
is present on the other hand, and $R_2^2 < R_1$, still a reduction in ES
heterogeneity is not inevitable. It can be argued that, for the
differences in ES to grow smaller, the reduction in relative
heterogeneity in score variance needs to be substantially large, so the
increase in ES heterogeneity due to the larger mean ES can be
“outpaced”. However, we did not observe this pattern in the 12 phenomena
that allowed for such an analysis.

Heterogeneity in ES even in direct replications, where experimental
factors are held as constant as possible, is already substantial
(Renkewitz, Fünderich, & Beinhauer, 2024). However, so far we have
demonstrated that in several scenarios, differences in score reliability
may in fact be masking, or “deflating” ES heterogeneity. This implies
that the already substantial heterogeneity across direct replications
may very well be even larger, if ES were corrected for (un)reliability.
Understanding heterogeneity as an extent to which a theory or phenomenon
is understood, deflated estimates of heterogeneity imply that the theory
or phenomenon is understood even worse than initially assumed.

The empirical arguments presented are based on a rather small number of
non-representative data-sets. Based on the combination of analytical and
empirical arguments, we are convinced that these differences in score
reliability across replications oftentimes mask true heterogeneity. In
realistic meta-analytic studies of conceptual replications, data stems
from a variety of sources. Across different populations, different
research settings or designs are used with varying or adjusted research
instruments. It is unlikely that, across these replications, we can
expect either the measuring quality or the true score variance to be
stable. However, as demonstrated above, differences in score reliability
and true score variance tend to lead to larger corrected ES
heterogeneity, implying that observed ES heterogeneity was reduced by
differences in score reliability. However, whether these results
actually generalize beyond these data-sets remains to be seen.
Unfortunately, the behavioural sciences are in dire need of open-data
that resemble multi-site replications. As the majority of results
discussed over the last years (in e.g. ManyLabs or Registered
Replication Reports) employ single-indicator scales as dependent
measures, score reliability can not be easily estimated in order to
replicate our analyses.

The analytical arguments presented following Equation 17 rested on the
use of the delta-method, as Equation 17 contained parameters of the
score standard deviations’ distributions, while the $R_1$ and $R_2$
metrics were designed to discuss score variances. The delta-method rests
on a number of assumptions: (1) approximations derived from the
delta-method require for the transformed variable to follow a marginal
normal distribution (ref). Since the variable under discussion here is
the standard deviation, which is bounded to be larger than zero and has
a non-normal sampling distribution (ref), this assumption is most likely
violated; (2) the derivative of the transformation function needs to be
available. As the transformation function is essentially just the square
root, this assumption should hold up; (3) as the delta-method
approximates the parameters of the transformed variable using the
first-order Taylor-expansion, this implies that higher-order terms are
required to be negligible. In practice, this implies that the variance
of the untransformed variable needs to be small. However, as this entire
procedure discusses the influence of variance in standard deviations on
ES heterogeneity, introduced by differences in score reliability, this
assumption is probably violated as well. The violation of these
assumptions likely introduce a bias in estimates derived from these
approximations (ref). However, the inequality derived with use of the
delta-method is not used to actually derive any estimates, but to
discuss how an attenuation correction procedure can affect ES
heterogeneity. Therefore, we believe that the violation of these
assumptions is defensible in this case, as the analytical arguments
derived from the inequality should still hold up, nevertheless.

Similarly, all Equations following Equation 10 rest on the assumption
that mean difference ($MD$) and true or total score variance
($\sigma^2_T$ and $\sigma^2_X$) are independent. While this assumption
is in line with assumptions underlying CTT, it is by no means guaranteed
that it holds in these data-sets. If this assumption was violated,
whether the relationship was positive or negative, indicated by a
positive or negative covariance in Equation 10, would most likely have
an impact on how strongly ES heterogeneity is affected by the
attenuation correction procedure, possibly even the direction of how it
is affected. However, currently we have no reason to assume that there
is a systematic relationship between mean difference and standard
deviation. As this relationship would also affect the distribution of ES
like Cohen’s d itself, strong violations of this assumption would go far
beyond invalidating the claims we derived in Equation 16. Instead,
assumptions of the meta-analytic tests concerning the size ES and the
presence of ES heterogeneity would be violated, making any research on
change in ES heterogeneity due to attenuation correction obsolete.

The work done by Hunter and Schmidt (2014) was crucial in informing and
guiding methodology development in the field of meta-analyses.
Similarly, Wiernik and Dahlke (2020) raise a number of important points
that have been neglected in the application of meta-analytic research
over the last decades. We strongly agree with their ideas that
underappreciated (differences in un-)reliability introduce substantial
biases in meta-analytic estimates and tests in Psychology, which need to
be corrected. Future applications of meta-analyses on standardized ES
need to take score reliability into account to arrive at correct
estimates and inferences. However, the meta-analyst should not expect
that correcting ES for score reliability will reduce the extent of
heterogeneity identified.


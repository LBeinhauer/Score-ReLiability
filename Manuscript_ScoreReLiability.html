<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Manuscript</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="Manuscript_ScoreReLiability_files/libs/clipboard/clipboard.min.js"></script>
<script src="Manuscript_ScoreReLiability_files/libs/quarto-html/quarto.js"></script>
<script src="Manuscript_ScoreReLiability_files/libs/quarto-html/popper.min.js"></script>
<script src="Manuscript_ScoreReLiability_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Manuscript_ScoreReLiability_files/libs/quarto-html/anchor.min.js"></script>
<link href="Manuscript_ScoreReLiability_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Manuscript_ScoreReLiability_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Manuscript_ScoreReLiability_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Manuscript_ScoreReLiability_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Manuscript_ScoreReLiability_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Manuscript</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In light of discussions surrounding the replicability of psychological results, meta-analytic heterogeneity is understood as a crucial parameter that may inform some aspects of (non-)replicability. Heterogeneity in itself helps understand why some replications of a single phenomenon may be successful, while other are not. If heterogeneity grows, meaning the phenomenon’s effect size varies more strongly for unexplained reasons, the probability of observing an effect size around zero or even in the negative space grows larger as well. If we know the true size and heterogeneity of a phenomenon’s effect size, we would theoretically be able to establish a baseline of expected replication rate (reference).</p>
<p>Similarly, it has been argued that heterogeneity in effect sizes is an indicator of the theory’s “completeness” surrounding the phenomenon. Linden and Hönekopp argue that “low (as opposed to high) heterogeneity reflects a more advanced understanding of the subject matter being studied” (2021, p.2). Similarly, Schuetze and von Hippel (2023) argue that heterogeneity in effects is an indicator of a vague, poorly specified theory. With these perspectives, heterogeneity in effect sizes can be understood as an indicator of the extent, to which the science surrounding a phenomenon has evolved to a sufficient understanding of that phenomenon.</p>
<p>Re-analyses of multi-site direct replications have demonstrated that heterogeneity tends to be the norm, if a psychological effect overall manages to replicate. Large-scale attempts of direct replications, using identical protocols, such as the Many Labs studies or Registered Replication Reports (references), for the first time allow researchers to estimate heterogeneity undistorted by typical experimental factors. In re-analyses of these studies, Olsson-Collentine et al.&nbsp;(2020) identify a strong correlation between a studies effect size and its heterogeneity. Similarly, van Erp et al.&nbsp;(2017) or Stanley et al.&nbsp;(2018) estimate strong degrees of heterogeneity across psychological replications in general. In a separate re-analysis of large scale direct replications, Renkewitz et al.&nbsp;(2024) identify substantial heterogeneity in almost all projects where a non-zero effect could be identified. This aligns with the correlation found by Olsson-Collentine et al.&nbsp;(2020).</p>
<p>In both the initial reports of large scale replication attempts, as well as the re-analyses by Olsson-Collentine et al.&nbsp;(2020), standardized effect sizes (ES), such as Cohen’s d or Hedge’s g were used. For the remainder of the article, Cohen’s d, defined in equation 1, will be used as an exemplary estimate of ES, as its used across a wide range of contexts and well understood by a broad audience.</p>
<p><span class="math display">\[d = \frac{MD}{\sigma_X}\]</span></p>
<p>Here, <span class="math inline">\(MD\)</span> refers to the difference between the two groups of interest, while <span class="math inline">\(\sigma_X\)</span> is the total pooled standard deviation.</p>
<p>As is widely known, score reliability affects such ES. In the context of classical test (CTT) theory score reliability is defined as the ratio of true to observed score variance, as defined in equation 2.</p>
<p><span class="math display">\[\rho_{XX’} = \frac{\sigma^2_T}{\sigma^2_X}\]</span></p>
<p>In this equation <span class="math inline">\(\sigma^2_T\)</span> refers to the true variance in the sense of CTT, meaning the actual variance of the variable, undistorted by measurement error. In the same sense, <span class="math inline">\(\sigma^2_X\)</span> refers to the total variance of scores, including both the true variance and the random error variance. Lower score reliability in a data-set leads to a smaller ES, opposed to a similar data-set with higher score reliability (references). In the meta-analytic context, as score reliability is an aspect of a measuring instrument applied to a population, score reliability may not be identical across replications. If score reliability varies across replications, meaning it also carries heterogeneity, it clearly should affect heterogeneity in observed ES as well.</p>
<p>Previous discussions of heterogeneity in score reliability have exclusively discussed it as a parameter that inflates heterogeneity in observed ES, implying that, if score reliability was perfect across all replications, the actual heterogeneity would have been lower. In their discussion, Wiernik and Dahlke claim that „<em>Measurement error variance will impact the results of meta-analyses in three ways: by (a) biasing the mean effect size toward zero, (b) inflating effect-size heterogeneity and confounding moderator effects, and (c) confounding publication-bias and sensitivity analyses</em>” (p.&nbsp;3, 2020). Additionally, more clearly, they state that “<em>If the studies included in a meta-analysis differ in their measures’ reliabilities, heterogeneity estimates will be artifactually inflated, erroneously suggesting larger potential moderator effects</em>” (p.&nbsp;4, Wiernik &amp; Dahlke, 2020). They base their claims largely on he work done by Hunter and Schmidt (2014), who claim that “<em>Variation in reliability across studies causes variation in the observed effect sizes above and beyond that produced by sampling error.</em>” (p.&nbsp;302). Overall, both references imply that differences in score (un)reliability inevitably lead to an inflated heterogeneity in ES.</p>
<p>However, if information concerning score reliability is available, it is possible to correct the individual ES for its unreliability. This process is also known as attenuation correction and, while not without its criticisms (reference), is a widespread practice in Psychology (reference). If the claims above, made by Wiernik and Dahlke or Hunter and Schmidt, were to be true, it would imply that corrections of individual observed ESs for their unreliability, any subsequently performed meta-analysis should generate lower estimates of heterogeneity. Hunter &amp; Schmidt even claim to have derived an equation which demonstrates how heterogeneity in score reliability inflates heterogeneity in ESs. “<em>If the level of reliability is independent of the true effect size across studies, then, to a close approximation</em>:” (p.&nbsp;309)</p>
<p><span class="math display">\[Var(\delta_0) = [E(a)]^2 Var(\delta) + [E(\delta)]^2 Var(a)\]</span></p>
<p>Here, <span class="math inline">\(\delta_0\)</span> refers to the “true” ES, undistorted by sampling error but not corrected for measuring error, <span class="math inline">\(\delta\)</span> refers to the “true” ES, undistorted by sampling error and measurement error and <span class="math inline">\(a\)</span> refers to score reliability in Schmidt and Hunter’s notation.</p>
<p>However, we propose that this understanding of how differences in score reliability affect the heterogeneity of observed ESs is incomplete. Hunter and Schmidt’s implication, that heterogeneity in score reliability inflates ES heterogeneity does not follow from equation 1. If equation 1 were to be true, it would imply that heterogeneity in <span class="math inline">\(\delta_0\)</span> is essentially a function where heterogeneity and mean value of <span class="math inline">\(\delta\)</span> are weighted by mean score reliability <span class="math inline">\(a\)</span> and its heterogeneity. In this scenario, it is easy to construct cases where heterogeneity in <span class="math inline">\(\delta_0\)</span> is inflated or deflated by heterogeneity in <span class="math inline">\(a\)</span>. In table 1, two such cases are demonstrated. All else is held equal, only the mean score reliability <span class="math inline">\(E[a]\)</span> is changed from .7 to .8. True heterogeneity <span class="math inline">\(Var[a]\)</span> is .2, at <span class="math inline">\(E[a] = .7\)</span> this leads to a smaller <span class="math inline">\(Var[\delta_0]\)</span>, while at <span class="math inline">\(E[a] = .7\)</span> this leads to a larger <span class="math inline">\(Var[\delta_0]\)</span>.</p>
<p>Parameter combinations of ES and score reliability, according to Hunter &amp; Schmidt (2014)</p>
<div class="cell">
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;"><span class="math inline">\(E[\delta]\)</span></th>
<th style="text-align: right;"><span class="math inline">\(Var[\delta]\)</span></th>
<th style="text-align: right;"><span class="math inline">\(E[a]\)</span></th>
<th style="text-align: right;"><span class="math inline">\(Var[a]\)</span></th>
<th style="text-align: right;"><span class="math inline">\(Var[\delta\_0]\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">0.7</td>
<td style="text-align: right;">0.198</td>
<td style="text-align: right;">0.2</td>
<td style="text-align: right;">0.100</td>
</tr>
<tr class="even">
<td style="text-align: right;">0.2</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">1.000</td>
<td style="text-align: right;">0.8</td>
<td style="text-align: right;">0.228</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>As Hunter &amp; Schmidt point out, equation 1 is only valid “<em>If the level of reliability is independent of the true effect size across studies […]”</em> (2014, p.&nbsp;309).</p>
<p>However, both parameters, ES and score reliability in their notation, can not be independent parameters by definition. ES is the “true” standardized mean difference, unaffected by sampling or measurement error. The ES is standardized using the total standard deviation, which is inflated by measurement error. Equation 1, demonstrating how Cohen’s d is computed demonstrates how the mean difference is standardized in this case. The ES on the other hand is standardized using the true standard deviation in the sense of CTT.</p>
<p><em>$$</em> <em>= \frac{MD}{\sigma_T}$$</em></p>
<p>In equation 4, is the true standard deviation, the square root of the true score variance found in equation 2. The fact that the true standard deviation parameter is found both in equation 4 and equation 2 demonstrates that ES and score reliability can not be independent variables. Therefore, the basic premise of equation 3 is not fulfilled, as the assumption of variable independence could not be fulfilled.</p>
<p><del>Hunter &amp; Schmidt’s discussion of how score reliability affects heterogeneity in ESs is fatally flawed [issues with the assumption underlying the equation].</del></p>
<p>Alternatively, assuming that both the mean differences and pooled standard deviations are no constant values across replications, meaning both carry non-zero heterogeneity, we can describe observed ESs as a random variable following a ratio distribution. Such a ratio distribution is the probability distribution, constructed by dividing one random variable by a second variable: $Z = \frac{X}{Y}$ (reference). If the distribution of the components is known, first order taylor approximation may be used to generate estimates of mean and variance of the ratio variable (reference). Using some additional assumptions, we use the Taylor-estimator of the ratio’s variance to demonstrate how differences in score reliability may affect heterogeneity in ES.</p>
<p>$$var[Z] \approx \frac{var[X]}{E[Y]^2} - \frac{2E[X]}{E[Y]^3} cov[X,Y] + \frac{E[X]^2}{E[Y]^4} var[Y]$$</p>
<p>Assuming that the random variable are uncorrelated, simplifies the equation to</p>
<p>$$var[Z] \approx \frac{var[X]}{E[Y]^2} + \frac{E[X]^2}{E[Y]^4} var[Y]$$</p>
<p>Substituting for ES, unstandardized mean difference MD and pooled standard deviation SD leads to</p>
<p>$$var[ES] \approx \frac{var[MD]}{E[SD]^2} + \frac{E[MD]^2}{E[SD]^4} var[SD]$$</p>
<p>Correcting ES for score reliability is technically done by correcting the pooled standard deviation: $SD_c = \sqrt{a V}$ . Since is always between zero and one, in the case of imperfect score reliability, SD_c will always be smaller than total SD. Additionally, if the variance in SD was introduced by differences in score reliability, these would be removed by the attenuation correction. However, there are two parameters that may introduce variance in SD across replications.</p>
<p>Firstly, as discussed, differences in score reliability may introduce variation to SD. Secondly however, the underlying latent variable we attempted to measure may have been not identically distributed across replications. If the populations where the replications are collected from are differently spread out in terms of the variable of interest, true differences in SD would remain. Re-analyses of the ManyLabs and Registered Replication Reports have shown that such differences in true variance across replications are the norm, not the exception. In that case, some variance in SD_c would remain, albeit smaller than the variance in SD. We reformulate equation X in terms of SD_c</p>
<p>$$var[ES_c] \approx \frac{var[MD]}{E[SD_c]^2} + \frac{E[MD]^2}{E[SD_c]^4} var[SD_c]$$</p>
<p>The implication of this equation is, again, not straightforward. The expected value of SD_c is guaranteed to be smaller than the expected value of uncorrected SD. Similarly, the variance in SD_c is guaranteed to be smaller than the uncorrected variance in SD. The expected value is always placed in the denominator of the function, while the variance is placed in the numerator of the second term in equation X+1. This implies that the variance in uncorrected ES can only be larger than the variance in ES_c, if the reduction in mean SD_c due to the attenuation correction increases the total equation result faster than the reduction in variance of SD_c decreases the total equation result. To illustrate this point, we consider some parameter combinations in table 2.</p>
<p><em>Table 2</em></p>
<p>Parameter combinations, leading to varying degrees of heterogeneity in ES_c, based on the Taylor approximation to the variance of a ratio distribution</p>

<table class="table">
<tbody>
<tr class="odd">
<td>
<p>
</p>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr class="even">
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
E[MD]
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
Var[MD}
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
E[SD]
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
Var[SD]
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
E[SD_c]
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
Var[SD_c]
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
Var[ES]
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
Var[ES_c]
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
</tr>
<tr class="odd">
<td>
<p>
</p>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr class="even">
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
1
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.2^2
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
1
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.2^2
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.8
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.1^2
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.08
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.087
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
</tr>
<tr class="odd">
<td>
<p>
</p>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr class="even">
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
1
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.2^2
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
1
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.2^2
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.8
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.05^2
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.08
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.069
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
</tr>
<tr class="odd">
<td>
<p>
</p>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr class="even">
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
1
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.2^2
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
1
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.2^2
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.8
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
0
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.08
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.063
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
</tr>
<tr class="odd">
<td>
<p>
</p>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr class="even">
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
1
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.2^2
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
1
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.2^2
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.9
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.1^2
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.08
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.064
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
</tr>
<tr class="odd">
<td>
<p>
</p>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr class="even">
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
1
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.2^2
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
1
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.2^2
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.7
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.1^2
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.08
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.124
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
</tr>
<tr class="odd">
<td>
<p>
</p>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr class="even">
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
1
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.2^2
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
1
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.2^2
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.5
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.1^2
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.08
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.32
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
</tr>
<tr class="odd">
<td>
<p>
</p>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
</tbody>

</table>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Table 2 demonstrates that in scenarios, where either score reliability was very high to begin with (.9 in this case), leading to very little change in E[SD] to E[SD_c] (row 5), or where the remaining variance in SD_c was comparably low (rows 3 and 4), the attenuation correction led to a reduction in ES heterogeneity. In most other scenarios, where score reliability was lower (.7 and .5 in rows 6 and 7) or remaining variance in SD_c sufficiently large (row 2), the attenuation correction actually increased the heterogeneity in ES. Note that in the table, we did not manipulate expected value or variance of MD or SD.</p>
<p>&nbsp;[implications of this distribution concerning Var[ES] and how that relates to Var[rel]]</p>
<p>Reformulating the approximation of variance in a ratio variable demonstrates how correcting for differences in reliability might affect the heterogeneity in ES.</p>
<p>Using analytical arguments from a reformulation of equation X we have demonstrated that differences in measuring quality often do not inflate estimates of heterogeneity. Instead, assuming that MD and SD are uncorrelated, we argue that in many circumstances differences in measuring quality deflate heterogeneity in standardized mean differences. This could be understood as a form of masking, where differences in measuring quality lead to a seemingly lower heterogeneity than we would have observed without any measurement error. Clearly, these arguments are in stark contrast with claims made by Hunter &amp; Schmidt or Wiernik &amp; Dahlke. To further illustrate this point, we turn towards an exemplary data-set, demonstrating that we can observe the deflation in heterogeneity in empirical data.</p>
<p><strong>Part C</strong></p>
<p>An exemplary data-set needs to fulfil the following conditions: i) it needs to contain several (direct/close or conceptual) replications of something that can be described as a two-group experimental effect; ii) the dependent variable needs to be measured using an identical scale across administrations, with multiple indicators/items/responses, so that estimates of score reliability can be derived; iii) there needs to be statistically significant heterogeneity in the standardized effect sizes; and iv) there needs to be statistically significant heterogeneity in the score reliability across administrations.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; While multi-site direct replication attempts, such as the Many Labs studies or Registered Replication Reports initially spring to mind, none of the studies replicated in these projects fulfil all four conditions. However, the Psychological Science Accelerator (PSA), even though their projects don’t have meta-analytic goals, provide data that may be transformed to fulfil all four conditions sufficiently. Specifically, we examined the data from PSACR002, one of the PSA’s studies concerning interventions and responses to the COVID-19 pandemic (reference). In the PSACR002 study, the authors assessed the effectiveness of reappraisal interventions on negative and positive emotion in participants grouped into four categories (reconstrual intervention, repurposing intervention, passive control, active control). The four groups may be collapsed into a single treatment group (reconstrual and repurposing) and a single control group (passive and active control). In total, Wang et al.&nbsp;(2021) assessed 21,644 individuals from 87 countries/regions. These countries/regions served as the study-level grouping variable for our meta-analyses.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Wang et al.&nbsp;(2021) used several measures which may be promising for this study. Out of four measures they use to assess negative and positive emotions, three should fit conditions ii), ii) and iv); positive and negative emotional responses to ten photographs shown in the experimental trial, positive and negative state emotion after viewing the ten photographs and positive and negative anticipated emotions following the study. Over the following paragraphs, we will briefly describe the study design and measures used that are of interest to us. Details regarding the protocol, stimuli or additional measures used may be found at […].</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; After randomization to one of four conditions, participants in the two intervention conditions received instructions on how to “change one’s thinking to change one’s emotions”, with more detail pertaining to the respective intervention. In the active control condition, participants were asked to reflect on their emotions, while in the passive control condition participants were asked to react naturally. Afterwards, the participants completed practice trials where they viewed two negative photographs in relation to the COVID-19 pandemic (e.g.&nbsp;people in hazmat suits, exhausted doctors), rated those photos on a Likert-scale from 1 to 5 on two questions: “how negative did the photo make you feel” and “how positive did the photo make you feel” (anchors: 1 = not at all, 5 = extremely). Depending on the condition the individual was randomized to, reminders of the strategy to be used were displayed.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; In the experimental trials, the participants viewed 10, similarly negative, photographs of the COVID-19 pandemic and rated their emotions on each photograph using Likert-style responses to the positive/negative questions. After the experimental trials, individuals responded to five negative items from the Differential Emotions Scale on fear, anger, sadness, distrust, and stress. Similarly, they responded to five positive items from the same scale, on hope, gratitude, love, inspiration, and serenity. Averages of the five positive and negative items respectively were used as measures of state emotions after the experimental trials. Similarly, negative and positive anticipated emotions were assessed, by prefacing a question of “In the next week, to what extent, if at all, do you think you will feel each of the following?” and following up with the same items used to assess state emotions. All items or the measures used may be found in the appendix XXX, or at [url…].</p>
<p><em>Table 3</em></p>
<p>Meta-analytic results concerning uncorrected and corrected standardized effect sizes across 3x2 measures of emotional response</p>

<table class="table">
<tbody>
<tr class="odd">
<td>
<p>
</p>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr class="even">
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
Measure
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
Valence
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
ES
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
Tau ES
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
ES_c
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
Tau ES_c
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
</tr>
<tr class="odd">
<td>
<p>
</p>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr class="even">
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
Photograph Response
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
Positive
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
1.065 (.065)
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.377 (sq .036)
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
1.219 (.080)
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.468 (sq .056)
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
</tr>
<tr class="odd">
<td>
<p>
</p>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr class="even">
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
Negative
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
-.663 (.040)
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.220 (sq .014)
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
-.747 (.047)
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.260 (sq .019)
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr class="odd">
<td>
<p>
</p>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr class="even">
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
Emotional State
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
Positive
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.213 (.018)
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.052 (sq .003)
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.234 (.020)
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.054 (sq .003)
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
</tr>
<tr class="odd">
<td>
<p>
</p>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr class="even">
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
Negative
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
-.203 (.020)
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.071 (sq .003)
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
-.221 (.022)
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.078 (.004)
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr class="odd">
<td>
<p>
</p>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr class="even">
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
Anticipated Emotion
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
Positive
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.159 (.015)
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
0 (sq .001)
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.172 (.016)
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
0 (.002)
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
</tr>
<tr class="odd">
<td>
<p>
</p>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr class="even">
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
Negative
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
-.146 (.020)
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.073 (sq .003)
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
-.145 (.022)
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
<p>
</p>
<p>
.079 (.004)
</p>
<p>
</p>
</td>
<td>
<p>
</p>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr class="odd">
<td>
<p>
</p>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
</tbody>

</table>
<p>In table 3 the results of meta-analyses on uncorrected and correct standardized effect sizes are summarised. In line with previous analytical arguments and the exemplary table of conditions, we observe that for effects of substantial size, meaning the absolute values of uncorrected ES for both photograph response measures, the heterogeneity in ES is increased by about 25% for the positive measures and about 20% for the negative measures after correcting for attenuation. For all other measures, where the difference in treatment and control groups is less strongly pronounced in terms of mean ES, heterogeneity also increases after correcting for attenuation, but a lot less strongly.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; In figure 1 a), the meta-analytical results for the positive photograph response are visualised in a forest plot. Grey dots represent the uncorrected ESs for each country, with accompanying confidence interval represented by the bars surrounding the dots. The overlying black dots on the other hand represent the corrected ESs, with additional confidence intervals.&nbsp; The position of the grey and black diamonds on the bottom display the meta-analytical estimate of mean ES, uncorrected (grey) and corrected (black), whereas the width of the diamonds represent the respective confidence interval. The forest plot shows that, as expected, the attenuation correction leads to larger ES and larger individual confidence intervals. Similarly, the meta-analytic estimate of mean ES is also larger, if score unreliability was corrected for, just like its accompanying confidence interval.</p>
<p><em>Figure 1</em></p>
<p>Forest plot and visualised underlying density for positive measures of Photographs</p>
<p><em>Note:</em> In a), grey dots represent individual uncorrected ES with grey bars representing the accompanying 95%-confidence intervals; black dots represent individual corrected ES wit black bars representing accompanying 95%-confidence intervals. In b), the grey density in the background represents the implied underlying distribution of uncorrected ES, while the black density in the foreground represents the implied underlying distribution of corrected ES.</p>
<p>Part b) in figure 2 visualises the implied underlying “true” distribution of ESs, free of sampling error. Essentially, the meta-analytic estimates of mean ES and its heterogeneity are used to construct a density for a normal distribution with these specific parameters. In part b), again, the grey density represents the distribution of uncorrected ES, while the black density represents the distribution of ES corrected for score unreliability. The higher spike in the grey demonstrates that the heterogeneity in uncorrected ES is smaller than the heterogeneity for corrected ES. Generally, the distribution of corrected ES is positioned further to the right, as the average ES is larger, as can also be seen in the forest plot. Additionally, as the density is more “flat” and with a lower spike, it shows that the heterogeneity in corrected ES is larger as well.</p>
<p><strong>Discussion</strong></p>
<p>Using both analytical arguments as well as an exemplary empirical dataset, we have demonstrated that differences in score reliability do not necessarily inflate heterogeneity in effect sizes. Inferences derived from the equation supplied in Hunter and Schmidt (ref) also do not warrant such a conclusion. Alternatively, describing standardised effect sizes in terms of a ratio distribution and approximating its variance, we have demonstrated that differences in score reliability may very well inflate or deflate variance in ES. In data supplied by the PSA (ref.) we have demonstrated that, across all measures, correcting observed ES for (un)reliability led to larger heterogeneity in ES than before correction.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; These results fit in well with work recently published/preprinted by Olsson-Collentine et al.&nbsp;(2023). In a large simulation scheme, they demonstrate that differences in score reliability across administrations initially deflate heterogeneity in correlations. Only as the true heterogeneity in correlations grows larger, score reliability differences actually inflate heterogeneity as predicted by Wiernik and Dahlke (xxx). While correlations and standardized effect sizes are not identical, the way score (un)reliability affects these parameters is highly similar. Both correlations and ES are deflated by (un)reliability, implying that an attenuation correction increases these parameters.</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Heterogeneity in ES even in direct replications, where experimental factors are held as constant as possible, is substantial (Renkewitz, Fünderich, &amp; Beinhauer, 2024). Previous assumptions on the role of score reliability differences, as discussed in Wiernik &amp; Dahlke (xxx), has been that they “inflate” heterogeneity. If these previous assumptions were true, it would imply that the true heterogeneity in such projects may be considerably smaller and less alarming.</p>
<p>However, so far we have demonstrated that in several scenarios, differences in score reliability may in fact be masking, or “deflating” ES heterogeneity. This implies that the already substantial heterogeneity across direct replications may very well be even larger, if ES were corrected for (un)reliability. Understanding heterogeneity as an extent to which a theory or phenomenon is understood, heterogeneity that was actually deflated implies that the theory or phenomenon is understood even worse than initially assumed. If the score reliability varies across administrations because the measuring quality is not identical across populations (Beinhauer, Fünderich &amp; Renkewitz, 2024), these differences additionally reflect the poor understanding of phenomenon or theory.</p>
<p>+ and – of this study</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The empirical arguments presented are based on a single study. While, based on the combination of analytical and empirical arguments, we are convinced that these differences in score reliability across replications oftentimes mask true heterogeneity, whether these results actually generalize beyond this data-set remains to be seen. Unfortunately, the behavioural sciences are in dire need of open-data that resembles multi-site replications. As the majority of results discussed over the last years (e.g.&nbsp;ManyLabs or Registered Replication Reports) employ single-indicator scales as dependent measures, score reliability can not be easily estimated in order to replicate our analyses.</p>
<p>How to go from here</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The analytical arguments presented in equation X could be improved by properly disentangling the terms concerning variance of SD and expected value of SD. As it stands now, the polynomial structure of the equation makes singling out an individual term rather difficult. However, in order to more easily understand in what way differences in score reliability lead to an inflation or deflation of heterogeneity, solving the equation for both the variance and expected value of SD would be highly beneficial.</p>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>
---
title: "Manuscript"
format: html
editor: 
  markdown: 
    wrap: 72
---

```{r, include = FALSE}
# library loading and installing as necessary


# relevant libraries required for this script
packages <- c("ggplot2", "here", "metafor", "gridExtra", "knitr", "magrittr", "dplyr")

# check, whether library already installed or not - install and load as needed:
apply(as.matrix(packages), MARGIN = 1, FUN = function(x) {
  
  pkg_avail <- nzchar(system.file(package = x))   # check if library is installed on system
  
  if(pkg_avail){
    require(x, character.only = TRUE)             # load the library, if already installed
    
  }else{
    install.packages(x, repos = "https://ftp.gwdg.de/pub/misc/cran/")                           # install the library, if missing
    require(x, character.only = TRUE)             # load after installation
  }
})

source(here("ReLiability_Function-library.R"))

ES_rma_df <- read.csv(here("Data/Processed/Aggregates_ES_analysis.csv"))

agg_L <- readRDS(here("Data/Processed/Aggregates_simple.csv"))

MASC_names <- unique(ES_rma_df$MASC)

```

# Liability or reliability? Exploring the role of effect size attenuation on meta-analytic heterogeneity

Large-scale collaborative replication efforts have sparked discussions
surrounding the replicability of psychological phenomena. For attempts
to estimate a phenomenon’s replicability or to predict whether a future
replication will replicate successfully, heterogeneity is a crucial
parameter that needs to be assessed. In the meta-analytic context,
heterogeneity describes the variation in effect sizes, free of sampling
error, if these are not constant across replications. Therefore, the
presence of heterogeneity implies that some replications of a single
phenomenon may be successful, while others are not. If heterogeneity
grows, meaning the phenomenon’s effect size varies more strongly for
unexplained reasons, the probability of observing an effect size around
zero or even in the negative space grows larger as well. If we know the
mean size and heterogeneity of a phenomenon’s effect, we would
theoretically be able to establish an expected replication rate
(reference).

Similarly, it has been argued that heterogeneity in effect sizes is an
indicator of the theory’s “completeness” surrounding the phenomenon.
Linden and Hönekopp argue that “low (as opposed to high) heterogeneity
reflects a more advanced understanding of the subject matter being
studied” (2021, p.2). Alternatively, Schuetze and von Hippel (2023)
argue that heterogeneity in effects is an indicator of a vague, poorly
specified theory.

Large-scale attempts of direct replications, using identical protocols,
such as the Many Labs studies or Registered Replication Reports
(references), for the first time allow researchers to estimate
heterogeneity undistorted by selection processes induced by publication
bias. In re-analyses of these studies, Olsson-Collentine et al. (2020)
identify a positive correlation between a phenomenon’s effect size and
its heterogeneity. Similarly, van Erp et al. (2017) or Stanley et al.
(2018) estimate strong degrees of heterogeneity across conceptual
replications in Psychology. In a separate re-analysis of large scale
direct replications, Renkewitz et al. (in preparation) identify
substantial heterogeneity in almost all projects where a non-zero effect
could be identified. This aligns with the correlation found by
Olsson-Collentine et al. (2020).

One source for this heterogeneity, which is not a typical moderator tied
to a specific theory, might be impairments in score reliability which
vary in their extent across replications. This is an argument put
forward repeatedly by original researchers and replicators alike.
Literature discussing the role of score reliability for standardized
effect sizes claims that differences in score reliability should
inflate, and therefore increase, heterogeneity. Over the following
pages, we assess whether score reliability can in fact be made
responsible for heterogeneity identified across phenomena. Even though
only a limited number of phenomena allow for such an analysis, we find
no empirical evidence for the argument that differences in score
reliability inflate effect size heterogeneity. Therefore, in the last
part of this text we turn towards an alternative discussion using
analytical arguments on how differences in measurement quality affect
standardized effect sizes and their differences.

## Effect sizes and score reliability in meta-analysis

In both the initial reports of large scale replication attempts, as well
as the re-analyses by Olsson-Collentine et al. (2020), standardized
effect sizes (from here-on abbreviated as ES) such as Cohen’s d or
Hedge’s g were used. For the remainder of the article, Cohen’s d,
defined in @eq-d, will be used as an exemplary estimate of ES, as it is
used across a wide range of contexts and well understood by a broad
audience.

$$d = \frac{MD}{\sigma_X}$$ {#eq-d}

Here, MD refers to the difference between two groups of interest, while
$\sigma_X$ is the total pooled standard deviation. Since $d$ constitutes
an observed effect size it is affected by sampling error. In terms of
heterogeneity, we are not interested in the variance of $d$, but the
variance of the true, underlying ES $\delta$. The standard error of
Cohen’s $d$, which is used to weight individual estimates in a
random-effects meta-analysis (reference) is computed as defined in
@eq-SEd.

$$SE_d=\sqrt{\frac{n_1+n_2}{n_1n_2}+\frac{d^2}{2\left(n_1+n_2\right)}}$$ {#eq-SEd}

As is widely known, score reliability affects ES. In the context of
classical test theory (CTT) score reliability $\rho_{XX’}$ is defined as
the ratio of true to total score variance, as defined in @eq-rel.

$$\rho_{XX’} = \frac{\sigma^2_T}{\sigma^2_X}=\frac{\sigma^2_T}{\sigma^2_T + \sigma^2_E}$$ {#eq-rel}

In this equation $\sigma_T^2$ refers to the true score variance in the
sense of CTT, meaning the actual variance of the variable, undistorted
by random measurement error. In the same sense, $\sigma_X^2$ refers to
the total variance of scores, including both the true variance and the
random error variance $\sigma_E^2$. This implies that, if true score
variance is assumed to be constant across replications, a lower score
reliability can only occur due to a larger error score variance. Total
score variance would also be larger, therein leading to a smaller ES, as
opposed to a similar data-set where a higher score reliability is
achieved through a smaller error score variance. However, score
reliability is an aspect of a measuring instrument applied to a
population. In the meta-analytic context, as studies tend to be
replicated in different populations, neither true nor error score
variance can be expected to always be identical across replications.
This most likely leads to heterogeneity in score reliability, which, as
discussed, is bound to affect ES heterogeneity as well.

Previous discussions of heterogeneity in score reliability have
exclusively discussed it as a parameter that inflates heterogeneity in
ES, implying that, if score reliability was perfect across all
replications, the actual heterogeneity would have been lower. In their
discussion, Wiernik and Dahlke claim that „*Measurement error variance
will impact the results of meta-analyses in three ways: by (a) biasing
the mean effect size toward zero, (b) inflating effect-size
heterogeneity and confounding moderator effects, and (c) confounding
publication-bias and sensitivity analyses*” (p. 3, 2020). Additionally,
more clearly, they state that “*If the studies included in a
meta-analysis differ in their measures’ reliabilities, heterogeneity
estimates will be artifactually inflated, erroneously suggesting larger
potential moderator effects*” (p. 4, Wiernik & Dahlke, 2020). They base
their claims largely on work done by Hunter and Schmidt (2014), who
claim that “*Variation in reliability across studies causes variation in
the observed effect sizes above and beyond that produced by sampling
error.*” (p. 302). Overall, both references imply that differences in
score (un)reliability inevitably lead to an inflated heterogeneity in
ES.

As both sources Hunter and Schmidt (2014) and Wiernik and Dahlke (2020)
will be referred to repeatedly, we abbreviate these references as H&S
and W&D respectively.

### Attenuation correction

If information concerning score reliability is available, it is possible
to correct the individual ES for its unreliability. This process is also
known as attenuation correction and, while not without its criticisms
(reference), is a widespread practice in Psychology (reference). @eq-dc
describes a simple attenuation correction procedure.

$$d_c = \frac{d}{\sqrt{\hat{\rho}_{xx’}}}$$ {#eq-dc}

Here, $d_c$ describes the attenuation-corrected estimate of $d$, which
is corrected by dividing it by the estimate of score reliability
$\hat{\rho}_{xx’}$. As described in W&D or Lord & Novick (references),
this is essentially the same, as if the corrected estimate of $d_c$ was
constructed using an estimate of true score standard deviation, as in
@eq-dct.

$$d_c = \frac{MD}{\hat{\sigma}_{T}}=\frac{MD}{\sqrt{\hat{\rho}_{xx’} \hat{\sigma}_X^2}}$$ {#eq-dct}

$\hat{\sigma}_T$ can be estimated by simply rearranging the terms from
@eq-rel. Thereby, an estimate of ES is generated, under the premise that
score reliability would have been perfect ($\rho_{xx'} = 1$). As the
corrected estimate of Cohen’s $d$ is larger and requires an additional
unknown parameter, an estimate of score reliability, the larger
uncertainty in this parameter should be reflected in a larger standard
error. The standard error for $d_c$ is defined in @eq-SEdc.

$$SE_{d_c} = \sqrt{SE^2_d \frac{d_c}{d}}$$ {#eq-SEdc}

The claims made in W&D imply that corrections of individual observed ESs
for their unreliability should lead to lower estimates of heterogeneity
in meta-analyses of corrected ES. If heterogeneity in score reliability
adds to ES heterogeneity, attenuation correction procedures should
eliminate that additional variation, as all corrected ES come with
identical (perfect) score reliability.

Re-analyses of the collaborative multi-site replication efforts have
uncovered differences in score reliability across replications of the
same phenomena (ref McShaw etc.). The claims made in W&D and H&S imply
that the ES heterogeneity uncovered in these projects therefore could
potentially be explained by these differences in score reliability.
Similarly, attempting to explain the substantial extent of ES
heterogeneity identified in psychological phenomena, both the original
authors as well as the multi-site replication authors claim that
differences in score reliability might be responsible (references).

Therefore, in the following section we assess to what extent this
intuition is supported by empirical evidence. The re-analysis of
archival data serves to shed light on the following two research
questions central to this text:

-   Can score reliability be used to explain heterogeneity in
    standardized effect sizes?

-   How does score reliability affect ES heterogeneity?

## Re-analysis of archival data

We have collected a large number of openly available multi-site direct
replications on psychological phenomena (Fünderich et al. 2024). The
data-sets of about 50 phenomena, largely stemming from the efforts of
the Many Labs studies, Registered Replication Reports or the
Psychological Science Accelerator, are made openly available in a
standardised format at \[osf.io\], the DRIPHT repository.

### Data

The ManyLabs projects were collaborative efforts to replicate several
psychological phenomena across different research sites, employing
identical protocols. From five published projects, the data for the
first three and the fifth Many Labs projects was made publicly available
when the DRIPHT repository was set up (references). The Registered
Replication Reports are similar collaborative efforts, with the sole
distinction that for each report a single phenomenon was replicated
several times. The Registered Reports 3-10 were added to the DRIPHT
repository, as they employed experimental designs with at least two
groups. Lastly, some projects from the Psychological Science Accelerator
were added to the DRIPHT repository. Different to Many Labs or
Registered Replication Projects, these collaborative efforts, while also
distributed across the globe, do not formally attempt to perform direct
replications, but focus on original research or conceptual replications.
However, as for each project, across all sites the same protocol is used
and the subjects are distributed across different countries, a data
structure similar to that of a multi-site replication study emerges.
What makes all these collaborative efforts valuable for this manuscript
is the fact that all phenomena collected in the DRIPHT-repository are
making use of two-group designs and are therefore easily assessed using
standardised effect sizes.

While replication data on a decent number of psychological phenomena is
available in the DRIPHT-repository, to demonstrate how incorporating
score reliability affects heterogeneity in ES, selected phenomena need
to fulfil three conditions: (1) As we focus on the use of ES $d$, the
phenomenon needs to be assessed using a two-group design. This is the
case for all projects in the DRIPHT repository; (2) Score reliability
estimates can be derived, this implies that the phenomenon needs to be
measured using either several indicators forming a single scale or by
repeatedly measuring across several timepoints; (3) Score reliability
can only attenuate effect sizes that are not zero in the first place.
Therefore, we focus on phenomena where meta-analytic mean ES can be
statistically distinguished from zero.

As all phenomena in the collection fulfil the first condition, 50
phenomena have been catalogued where the effect is studied by comparing
the outcome across two separate groups. From the collection of 50
phenomena, 21 fulfil the second condition, so that estimates of score
reliability can be derived. For the 21 phenomena, the dependent variable
used to construct the effect size is measured by employing more than a
single indicator. At the same time, however, this implies that the
remaining 29 phenomena did not make use of more than a single indicator
to measure the dependent variable. Therefore, for the majority of
phenomena it is not possible to assess in how far measurement quality is
sufficiently high in terms of e.g. internal consistency or low random
error variance.

From the 21 phenomena that employed more than a single indicator, 19
made use of Likert-style items, where respondents would indicate their
agreement to some kind of statement. The scale-length varied from 3 to
10-points. The items measuring the remaining 2 phenomena were coded
dichotomously, as responses were right or wrong. While the majority of
designs make use of some form of control-treatment manipulation,
randomly assigning participants to a condition (20 phenomena), some
phenomena refer to the effect of pre-existing differences, e.g.
biological sex (2 phenomena). Subsequently, those phenomena need to be
identified where the meta-analytic mean effect size is statistically
significant different from zero. This will be done as a preliminary step
in this manuscript’s analysis procedure. Detailed information concerning
the different phenomena and how they were measured can be found at
\[osf-link\], while a brief summary can be found in Table 1.

To assess whether ES heterogeneity can be reduced by correcting for
score reliability, estimates of meta-analytic heterogeneity are compared
across two situations: first, heterogeneity of raw ES, as computed in
equation 1 is assessed. Secondly, ES are corrected for imperfect
reliability. Computing the heterogeneity of these corrected ES allows
for an assessment, whether the heterogeneity did indeed shrink compared
to the first situation, as predicted in W&D and H&S.

### Methods

For each replication of a phenomenon, Cohen’s $d$ is computed as an
estimate of raw ES, according to @eq-d, including its standard error
(@eq-SEd). Subsequently, a random-effects meta-analysis is performed
using metafor version X.XX. To estimate meta-analytic mean ES and
heterogeneity, the REML-estimator is used (ref). To identify which
phenomena pass criterion (3), using a Wald-type significance test we
assess for which phenomenon the meta-analytic mean ES is significantly
different from zero. For this hypothesis test, as all other hypothesis
tests in this manuscript, a one-tailed significance level of .05 is
used.

Additionally, estimates of score reliability are derived. While not
without criticism (references), Cronbach’s Alpha is used to estimate
score reliability. It is often noted that tau-equivalence is an
unrealistic assumption to hold against real-world data, implying that
other estimates of score reliability, such as McDonald’s Omega
(reference), Guttman’s Lambda 2 or 4 (reference), or the Greatest Lower
Bound (reference) might be better suited. However, all scales employed
in this project are scored by computing the simple mean or sum of
responses across items for each individual. Computing the mean or sum of
several items implicitly assumes tau-equivalence, as each item
contributes equally to the individual’s test score. Therefore,
Cronbach’s Alpha is the better choice to estimate score reliability, as
it avoids a mismatch in assumptions between how the score is computed
and how score reliability is estimated. Using these estimates of
Cronbach’s Alpha, the individual estimates of Cohen’s d are corrected,
according to @eq-dc, including the corrected standard errors (@eq-SEdc).
Thereby, estimates of ES are computed, which are corrected for imperfect
reliability. Again, a random-effects meta-analysis is run using metafor,
generating an estimate of heterogeneity in corrected ES. Different
indicators of heterogeneity are readily available, such as $I^2$, $H^2$
or the coefficient of variation. However, the descriptions found in H&S
and W&D discuss the absolute amount of heterogeneity in terms of
variance ($\tau^2$) or standard deviation ($\tau$). These parameters
discuss the variability of the standardized ES in the population in
terms of variance or standard deviation. Therefore, we also make use of
absolute heterogeneity here.

Subsequently, in order to assess whether ES heterogeneity was indeed
reduced by the attenuation correction procedure, the estimates of
heterogeneity in uncorrected ES and corrected ES are compared. Using
Cochran’s Q-test, we assess whether the estimates of heterogeneity are
statistically significantly different from 0. Only for those projects
where we have sufficient confidence that the individual estimates of
heterogeneity are larger than zero can we actually begin to interpret
whether there was any change.

To identify whether a reduction in ES heterogeneity is indeed
accompanied by differences in score reliability, the estimates of score
reliability are assessed meta-analytically as well. To do so, a
Reliability Generalization Meta-Analysis is performed (references). This
entails that individual estimates of score reliability are adequately
transformed using Bonett’s transformation:
$T_{\hat{\rho}} = ln(1 - \hat{\rho})$. This transformation has
variance-stabilising properties and therefore allows for adequate
inferences concerning heterogeneity in score reliability (references).
Cochran’s Q-test is used to identify statistically significant
heterogeneity in transformed score reliability. Estimates derived from a
Reliability Generalization Meta-Analysis using these transformations can
be back-transformed to the original score reliability scale
(references):

$$\mu_{\rho{xx'}} \approx 1 - exp(\mu_{T\left[\rho_{XX^\prime}\right]})-\frac{1}{2} exp(\mu_{T\left[\rho_{xx'}\right]})\tau_{T\left[\rho_{XX^\prime}\right]}^2$$ {#eq-rel_back_mu}

$$\tau_{\rho_{xx'}}^2 \approx exp(\mu_{T\left[\rho_{xx'}\right]})2 \tau_{T\left[\rho_{xx'}\right]}^2+exp(\mu_{T\left[\rho_{xx'}\right]})^2\tau_{T\left[\rho_{xx'}\right]}^4$$ {#eq-rel_back_tau}

These procedures are followed separately for each phenomenon. The
statistical programming language R, Version X.XXX (reference) is used
for all data manipulation and statistical analysis.

### Results - Wich phenomena pass criterion 3)?

[Equations @eq-d], [-@eq-SEd], [-@eq-dc], and [-@eq-SEdc] were used to
generate estimates of standardized ES, both uncorrected and corrected,
with their corresponding standard errors. Generally, this leads to
larger (absolute) ES and larger standard errors. As an example from the
21 selected phenomena, the results from this procedure on Nosek’s
Explicit Art sex differences phenomenon are displayed in the forest plot
in @fig-forest. Here, grey dots represent the uncorrected ES for each
sample, while black dots represent the corrected ES in each sample. The
bars surrounding these dots show the respective 95%-Cis.

```{r, echo = FALSE}
#| label: fig-forest
#| fig-cap: Forest Plot Nosek_Explicit_Art
forest_plot_rel(ES_rma_df[which(ES_rma_df$MASC == "Nosek_Explicit_Art"),], agg_L[[which(MASC_names == "Nosek_Explicit_Art")]])
```

In @fig-forest it becomes apparent that the reliability attenuation
procedure leads to an increase in the individual absolute effect size,
as the black dots are moved further away from zero. Simultaneously, the
standard errors grew larger after the attenuation correction, which
leads to larger 95%-CIs. This is most easily observed for the
mturk-sample, which already had a rather large confidence interval to
begin with. Additionally, in the diamond at the bottom of the figure, we
can see that the meta-analytic estimate mean ES is also larger after the
attenuation correction took place. In @tbl-meanES, the estimates and
tests on the meta-analytic average ES, both corrected and uncorrected,
can be found.

@tbl-meanES demonstrates that what we observed in Figure 1 holds across
all 21 phenomena. All meta-analytic effects are larger in their absolute
size after applying an attenuation correction procedure. Similarly, the
uncertainty quantified in the estimate's standard error is larger.
Additionally, @tbl-meanES highlights which phenomena pass criterion (3)
– the effect size must be statistically significantly distinguishable
from zero. 12 phenomena pass this criterion, highlighted in boldface. It
may be remarked that the attenuation correction procedure, in this data,
appears to have no influence on whether a phenomenon passes the
criterion. The differences in p-value are rather small and lead to no
difference in conclusion, regarding a significance level of .05.

```{r, echo = FALSE}
#| label: tbl-meanES
#| tbl-cap: Meta-analytic average  ES
knitr::kable(x = read.csv(here("Tables/Mean_ES.csv")))
```

### Results - Estimates of heterogeneity

@tbl-tauES contains the results from performing an RG-MA on the
Cronbach's Alpha coefficients of the scales used to measure the 12
remaining phenomena. Additionally, it contains estimates of
heterogeneity for both uncorrected ES ($\delta_0$) and corrected ES
($\delta$), which are central to assess the research questions posed in
section XXX. Most importantly, @tbl-tauES demonstrates that for all but
one of the phenomena, the attenuation correction procedure barely has an
impact on the extent of ES heterogeneity. For all phenomena but
Srull_Behaviour_Hostility, the heterogeneity in ES before and after the
correction changes very little. This implies that for 11 out of 12
phenomena, differences in score reliability can not be made responsible
for heterogeneity in standardized effect sizes. Only for the
Srull_Behaviour_Hostility phenomenon, accounting for score reliability
actually does reduce, and therefore explain, ES heterogeneity.

```{r, echo = FALSE}
#| label: tbl-tauES
#| tbl-cap: Tests for ES Heterogeneity
#knitr::kable(x = read.csv(here("Tables/Heterogeneity_ES.csv")))

tab_hetES <- read.csv(here("Tables/Heterogeneity_ES.csv"))
tab_hetrel <- read.csv(here("Tables/Results_RMA_alpha.csv"))


tab_both <- data.frame(tab_hetES$MASC,
                       tab_hetrel[,c("mu_alpha_str", "tau_alpha", "QE_str", "QEp")],
                       tab_hetES[,c("tau_raw", "QE_raw_str", "QEp_raw", "tau_cor", "QE_cor_str", "QEp_cor")])


knitr::kable(tab_both,
             col.names = c("MASC", 
                           "$\\hat{\\mu}_{\\rho_{XX'}}$ (95%-CI)", 
                           "$\\hat{\\tau}_{\\rho_{XX'}}$", 
                           "QE (df)", "p", 
                           "$\\hat{\\tau}_{\\delta_0}$", 
                           "QE (df)", "p",
                           "$\\hat{\\tau}_{\\delta}$", 
                           "QE (df)", "p"),
             escape = FALSE)
# 
# tab_one <- data.frame(tab_muES,
#                       tab_hetrel[,c("mu_alpha_str", "tau_alpha", "QE_str", "QEp")])
```

*Note:* $\hat{\mu}_{\rho_{XX'}}$ is the back-transformed meta-analytic
estimate of mean score reliability; $\tau{\mu}_{\rho_{XX'}}$ is the
back-transformed estimate of heterogeneity in score reliability; Note
that, since in a meta-analysis, we attempt to estimate mean and variance
of the parameter's true distribution, in this table
$\hat{\mu}_{\delta_0}$ represents the meta-analytic estimate of mean
uncorrected $d$ and $\hat{\tau}_{\delta_0}$ represents the estimate of
its heterogeneity; accordingly, $\hat{\mu}_{\delta}$ represents the
estimate of the meta-analytic mean of corrected $d_c$ and
$\hat{\tau}_{\delta}$ the estimate of its heterogeneity.

Secondly, in @tbl-tauES we find that for ten out of twelve phenomena, ES
heterogeneity grows as a result from the attenuation correction
procedure. Even though these differences tend to be small, it is clear
that in all those cases, not only did accounting for score reliability
not explain any ES heterogeneity, we find even more than initially
expected. This is incompatible with claims made in W&D and H&S, the
attenuation correction increased ES heterogeneity. Concerning the
remaining two phenomena, for the Shnabel_Willingness_Reconcile_Rev, the
reduction in heterogeneity is miniscule and the estimate of
heterogeneity is not statistically significant in the first place. Most
likely, the difference in ES heterogeneity estimates can be explained by
issues of estimation, as only eight replications are available.
Similarly, only for the Srull_Behaviour_Hostility phenomenon, we find a
substantial reduction in heterogeneity due to the attenuation correction
procedure. The estimates of ES heterogeneity are not statistically
significant, however, which hampers our ability to meaningfully
interpret and compare these estimates. For the majority of phenomena it
appears that differences in score reliability do not inflate
heterogeneity in uncorrected effect sizes, but instead mask
heterogeneity, that can be unveiled by accounting for score reliability.

## Relationship between score reliability and effect sizes in terms of heterogeneity

In the previous section we demonstrated that assumptions formulated in
the literature regarding meta-scientific methodology are false. We found
a large number of phenomena where, contrary to popular claims, score
reliability does not explain ES heterogeneity. Over the following
paragraphs, we discuss how the true relationship between these two
parameters might look like. Therein, we also identify under which
circumstances an attenuation correction can actually reduce ES
heterogeneity - and where it is more likely to inflate it.

### Hunter & Schmidt's misinterpreted equation

Hunter & Schmidt claim to have derived an equation which demonstrates
how heterogeneity in score reliability inflates heterogeneity in ESs.
“If the level of reliability is independent of the true effect size
across studies, then, to a close approximation:” (p. 309). However, as
can be easily verified, the equation presented in Hunter & Schmidt
(ref.), Equation X on p. 309, does not imply that assumption across all
conditions.

$$\tau^2_{\delta_0} = [\mu_{\rho_{xx‘}}]^2 \tau^2_{\delta} + [\mu_{\delta}]^2 \tau^2_{\rho_{xx‘}}$$ {#eq-HS}

We adjusted @eq-HS using the notation used throughout this text. Here,
$\delta_0$, carrying a 0 in the index, refers to the uncorrected ES,
undistorted by sampling error but not corrected for measuring error. On
the other hand $\delta$, no index, refers to the corrected ES,
undistorted by sampling error and measurement error. Lastly,
$\rho_{XX'}$ refers to score reliability.

@eq-HS implies that heterogeneity in $\delta_0$ is essentially a
function in which heterogeneity and mean value of $\delta$ are weighted
by mean score reliability $\rho_{xx‘}$ and its heterogeneity. Using
selected values for the parameters in @eq-HS, we demonstrate how the
heterogeneity in standardized ES can be both inflated and deflated by
the presence of differences in score reliability. In @fig-hs, the
results of employing @eq-HS to compute the heterogeneity (variance) in
uncorrected ES $\delta_0$ is highlighted. All else held equal, only the
individual level of mean reliability $\mu_{\rho_{xx‘}}$ is varied from
.5 to .8. Mean corrected ES $\mu_{\delta_0}$ is held constant at 1, its
heterogeneity $\tau_{\delta_0}$ held constant at .2 and the
heterogeneity of score reliability $\tau_{\rho_{xx‘}}$ held constant at
.12.

```{r, echo = FALSE}
#| label: fig-HS
#| fig-cap: Implications by Hunter & Schmidt's @eg-HS \n Note: The red line corresponds to the variance of uncorrected ES $\delta_0$.

library(ggplot2)

mu_delta0 <- 1
var_delta0 <- .2^2
mu_alpha <- seq(from = .5, to = .8, length.out = 100)
var_alpha <- .12^2
mu_delta <- mu_delta0/sqrt(mu_alpha)

# var_delta0 <- mu_alpha^2 * var_delta + mu_delta^2 * var_alpha
var_delta <- (var_delta0 - mu_delta^2 * var_alpha) / (mu_alpha^2)

ggplot() +
  geom_hline(yintercept = .04, colour = "red", linewidth = 1, linetype = "dashed") +
  geom_line(aes(y = var_delta, x = mu_alpha), linewidth = 2) +
  theme(panel.background = element_rect(fill = "transparent"), 
        plot.background = element_rect(fill = "transparent", colour = "transparent"), 
        panel.grid.major.y = element_line(colour = "grey"),
        panel.grid.major.x = element_line(colour = "grey"),
        panel.grid.minor = element_line(colour = "transparent"),
        axis.ticks = element_line(colour = "grey"),
        strip.background = element_rect(fill = "transparent")) +
  labs(y = expression(tau^2~"["~delta~"]"),
       x = expression(mu~"["~rho[xx]~"]"))
```

If everything else is held constant, the variance in corrected ES
$\delta$ can be a function only of the mean level of score reliability
$\mu_{\rho_{xx‘}}$, according to H&S equation. In that case, Figure 2
demonstrates that correcting for imperfect score reliability does not
inevitably lead to lower heterogeneity, as the line representing
heterogeneity in corrected ES does cross the red line, representing the
heterogeneity in uncorrected ES. Depending on the mean level of score
reliability, heterogeneity in uncorrected ES can be higher or lower than
heterogeneity in corrected ES.

However, @eq-hs is additionally flawed, as it violates a crucial
definition. As Hunter & Schmidt point out, @eq-hs is only valid “If the
level of reliability is independent of the true effect size across
studies \[…\]” (2014, p. 309). However, if we assume that true score
variance $\sigma^2_T$ varies across administrations, it is unlikely that
this assumption can hold. As demonstrated further above in @eg-dct, ES
$\delta$ is standardized using the true standard deviation $\sigma_T$ in
the sense of CTT. The fact that this parameter is found both in @eq-dct
and @eq-rel demonstrates that ES $\delta$ and score reliability
$\rho_{xx‘}$ are unlikely to constitute truly independent variables, as
long as true score variance is not identical across administrations.

### Describing ES as a random ratio variable

However, realising that an ES, as defined in @eq-d, is actually a ratio,
allows for a different analytical description of how differences in
score reliability can affect ES heterogeneity. If the phenomenon is
heterogeneous and score reliability varies across replications, it seems
sensible to assume that both numerator and denominator (MD and
$\sigma_X$) from @eq-d vary across replications. In that case, an ES can
be described as a random variable stemming from a ratio distribution. A
ratio distribution is a probability distribution constructed by dividing
one random variable by a second random variable: $Z = \frac{X}{Y}$
(reference). If the distribution of the components is known, first order
taylor approximation may be used to generate estimates of mean and
variance of the ratio variable (reference). Here we use the
Taylor-estimator of the ratio’s variance to demonstrate how differences
in score reliability may affect heterogeneity in ES.

$$\tau^2[Z] \approx \frac{\tau^2[X]}{\mu[Y]^2} - \frac{2\mu[X]}{\mu[Y]^3} cov[X,Y] + \frac{\mu[X]^2}{\mu[Y]^4} \tau^2[Y]$$ {#eq-XYZ_cov}

Assuming that the random variables are uncorrelated, simplifies the
equation to

$$\tau^2[Z] \approx \frac{\tau^2[X]}{\mu[Y]^2} + \frac{\mu[X]^2}{\mu[Y]^4} \tau^2[Y]$$ {#eq-XYZ}

Substituting for ES, unstandardized mean difference MD and pooled
standard deviation $\sigma_X$ leads to

$$\tau^2[\delta_0] \approx \frac{\tau^2[MD]}{\mu[\sigma_x]^2} + \frac{\mu[MD]^2}{\mu[\sigma_x]^4} var[\sigma_x]$$ {#eq-ratio_d0}

As stated in @eq-dct, correcting ES for score reliability can be
described through a correction of the pooled standard deviation:
$\sigma_T = \sqrt{\rho_{xx’} \sigma^2_x}$. Since $\rho_{xx'}$ is always
between zero and one, in the case of imperfect score reliability,
$\sigma_T$ will always be smaller than total $\sigma_x$. Additionally,
if the heterogeneity in $\sigma_X$ was introduced purely by differences
in measurement precision – the error score variance $\sigma_E^2$, this
would be removed by the attenuation correction. However, alternatively,
the underlying latent variable we attempted to measure may not be
distributed identically across replications. In that case, true score
variance $\sigma^2_T$ would vary across replications, leading to
differences in both score reliability, corrected ES and raw ES.
Importantly, this implies that even if measurements of perfect score
reliability were taken across the different replications, heterogeneity
in ES would still persist.

In that case, some heterogeneity in $\sigma_T$ would remain, albeit
heterogeneity found in $\sigma_x$ was reduced. @eq-ratio_d0 essentially
demonstrates how mean and heterogeneity of MD, paired with mean and
heterogeneity of total score standard deviation $\sigma_x$ can be used
to estimate heterogeneity in raw ES $\delta_0$. Similarly, we can adjust
Equation @eq-ratio_d0 to describe how, instead of total score standard
deviation, mean and variance in true score standard deviation $\sigma_T$
affect heterogeneity in corrected ES $\delta$.

$$\tau^2[\delta] \approx \frac{\tau^2[MD]}{\mu[\sigma_T]^2} + \frac{\mu[MD]^2}{\mu[\sigma_T]^4} \tau^2[\sigma_T]$$ {#eq-ratio_d}

The differences between the two [Equations @eq-ratio_d0] and
[-@eq-ratio_d] demonstrates how ES heterogeneity is affected by an
attenuation correction procedure. According to the quotes in H&S and
W&D, @eq-ratio_d0 should lead to a larger value of
$\tau^2\left[\delta_0\right]$, compared to the estimate of
$\tau^2\left[\delta\right]$ from @eq-ratio_d. However, the equations
shown throughout this text do not agree with this conclusion.

Instead, what is guaranteed is that the expected value of $\sigma_T$ is
smaller than the expected value of $\sigma_X$. Similarly, the
heterogeneity in $\sigma_T$ is guaranteed to be smaller than the
uncorrected heterogeneity in $\sigma_X$. However, since the expected
value of either standard deviation is placed in the denominator of the
function and the variance of either is placed in the numerator of the
function, @eq-hs is not sufficient to explain how heterogeneity in ES
changes due to the attenuation correction.

In this section, we attempt to understand under which circumstances the
heterogeneity in ES grows larger as the result of an attenuation
correction procedure, contrary to claims made in W&D and H&S. To do so,
we propose the following inequality, involving [Equations @eq-ratio_d0]
and [-@eq-ratio_d].

$$\tau^2_{\delta} > \tau^2_{\delta_0} \quad  \quad \quad \quad \quad  \frac{\tau_{MD}^2}{\mu_{\sigma_T}^2} + \frac{\mu_{MD}^2}{\mu_{\sigma_T}^4} \tau^2_{\sigma_T} > \frac{\tau_{MD}^2}{\mu_{\sigma_x}^2} + \frac{\mu_{MD}^2}{\mu_{\sigma_x}^4} \tau^2_{\sigma_x}$$ {#eq-ineq_init}

As long as the inequality defined in @eq-ineq_init holds true, an
attenuation correction procedure would lead to larger estimates of ES
heterogeneity. Conditions under which @eq-ineq_init does not hold are
conditions under which the claims made in W&D and H&S are met. However,
the inequality in this form does not convey sufficient information about
the conditions under which it can potentially hold. In Appendix A, we
rearranged @eq-ineq_init, so we end up with a form that we can use to
actually derive conditions under which ES heterogeneity can be inflated
or deflated by differences in score reliability.

To do so, we introduce two additional metrics.

$$R_1 = \frac{\mu_{\sigma^2_T}}{\mu_{\sigma^2_X}}$$ {#eq-R1}

$R_1$ describes the relative size of mean true score variance, compared
to the mean observed score variance. This informs us, how much of the
mean total score variance can be attributed to mean true score variance.
Therefore, this metric is close to the average score reliability. Even
though formally $R_1$ and the meta-analytic mean score reliability
$\mu_{\rho_{xx'}}$ do not share not the same value, in practice we
observe little to no differences between the two parameters. Large
values indicate that correcting the individual score variances (or
standard deviations for that matter) using the attenuation correction
procedure should lead to very little change in ES, as most of the
variation observed were due to genuine differences in true scores. Small
values indicate the opposite, the observed score variance was due to a
larger amount of random error score variance, leading to a stronger
correction.

$$R_2 = \frac{\tau^2_{\sigma^2_T}}{\tau^2_{\sigma^2_X}}$$ {#eq-R2}

The metric $R_2$ on the other hand describes in how far the attenuation
correction procedure has successfully reduced heterogeneity in score
variance $\tau^2_{\sigma^2_X}$. A value of 1 indicates that the
heterogeneity in true score variance $\sigma^2_T$ is essentially just as
large as the heterogeneity initially observed. Smaller values indicate
how much heterogeneity is "left", after applying an attenuation
correction, relative to the initial heterogeneity. This means that a
value of .7 indicates that about 70% of heterogeneity in score variance
remains, even after correcting for differences in score reliability. As
only 30% of score variance heterogeneity could be removed, this would
imply that differences in error score variance $\sigma^2_E$ were
responsible for less than a third of the heterogeneity in score
variances found. On the contrary, more than two thirds of heterogeneity
could be attributed to actual differences in how the underlying true
scores are distributed across samples.

Concerning the re-arranged inequality from Appendix A, we identify one
crucial condition or rule described in @eq-ineq_short.

$$R_1^3 \leq R_2$$ {#eq-ineq_short}

As long as $R_1^3$ is smaller or equal compared to $R_2$, @eq-ineq_init
is bound to hold true. This means that as long as the average score
reliability, to the power of 3, does not exceed the relative remaining
variance heterogeneity after the attenuation correction, @eq-ineq_init
is not violated. For example, assuming the average score reliability is
about .8, as long as $R_2$ is at least .512 or larger, the inequality
holds true and ES heterogeneity grows larger as a consequence of the
attenuation correction procedure. In such cases, the claims made in W&D
and H&S could not hold.

$R_1$ essentially describes the relative size of mean score variance
after the attenuation correction, and $R_2$ essentially described the
relative size of score variance heterogeneity after the attenuation
correction. Therefore, @eq-ineq_init states that for such a correction
procedure to truly reduce ES heterogeneity, the reduction in the
heterogeneity of score variance needs to be substantially larger than
the reduction in its mean values. The mean value reduction carries
stronger weight, as $R_1$ is taken to the power of 3 in @eq-ineq_init.
The smaller $R_1$ or $R_2$, the larger the reduction in heterogeneity or
mean value.

As long as there are substantial differences in the true score variance
across administrations, it is unlikely that an attenuation correction
can actually produce such a low $R_2$. Generally speaking, $R_1$, and
therefore the reduction in mean score variance, largely depends on the
relative measurement precision. If measurements were made more
precisely, average score reliability and $R_1$ would be higher. On the
other hand $R_2$, and therefore the reduction in heterogeneity of score
variance, depends on how different the measurement precision was at the
individual administrations, but also how different the individual
administrations sites are regarding their true score variance. If the
administration sites carry large differences in true score variance
$R_2$ is unlikely to be substantially smaller than $R_1$.

It is important to note that the condition in @eq-ineq_short only
describes under which conditions the inequality is guaranteed to hold,
no matter what value other parameters like the mean value or
heterogeneity in mean differences take on. This means that, even if the
condition in @eq-ineq_short is violated, it is by no means guaranteed
that an attenuation correction procedure would actually reduce
heterogeneity in ES. This means that inference made from @eq-ineq_short
is asymmetrical. While we know, if the inequality holds, heterogeneity
in corrected ES will be larger than heterogeneity in uncorrected ES, we
can't be certain that the opposite is true if the inequality does not
hold.

***\[maybe move to discussion further below\]*** The majority of studies
in the social sciences make use of convenience sampling. Often,
respondents are collected from a local student pool, where students
themselves would choose which studies to partake in. In such cases, we
believe there is little reason for us to expect homogeneity in true
score variance across administration sites. As participants across
administrations self-select into the study, no real sampling process
takes place - implying that we have no reason to assume that the true
score variance is stable across these administration sites. However,
since researchers are well aware of the importance of measurement
precision, which is also held stable more easily, we believe there is
little reason for $R_1$ to be extremely low.

### Re-analysis of archival data

@tbl-tauES showed that for all phenomena, where we identified
statistically significant ES heterogeneity, the heterogeneity was even
larger after the attenuation correction procedure. Subsequently, in
Appendix A we demonstrate that if the $R_2$ metric is larger than
$R_1^3$, ES heterogeneity is bound to be larger after applying said
procedure. Here, we will re-analyse the data, estimating $R_1$ and $R_2$
for each phenomenon respectively. This allows us to assess in how far
the estimates of ES heterogeneity align with our claims regarding the
$R_1$ and $R_2$ metrics.

#### Methods

Estimation of $R_1$ and $R_2$ require estimates of meta-analytic mean
and heterogeneity of both the total score variance $\sigma^2_X$ and the
true score variance $\sigma^2_T$. Elsewhere, we proposed the method
boot-err (ref.) as an alternative method to test for and estimate the
differences in error score variance $\sigma^2_E$. In Beinhauer et al.
(ref.), we deliver detailed instructions on how estimates for mean and
heterogeneity of total score variance, and the score variance components
can be derived. For the total score variance $\sigma^2_X$, all we need
to do is to appropriately transform the observed estimates using the
variance-stabilizing properties of the natural log. Subsequently, we use
bootstrapping to derive robust standard errors for these transformed
observations. Using a random-efffects meta-analysis, we derive estimates
of mean and heterogeneity, which need to be backtransformed to the
original scale. Appropriate equation to back-transform estimates can be
found in Appendix B.

For the true score variance $\sigma^2_T$, we found that directly
estimating its heterogeneity, even if an appropriate
variance-stabilizing transformation is used, is severely biased.
Instead, we can estimate the error score variance $\sigma^2_E$ using an
estimate of score reliability. Performing the boot-err, which, again,
implies transformations using the natural log, deriving standard errors
using Bootstrapping, running a random-effects meta-analysis and
back-transforming the estimates, nets us estimates of mean and
heterogeneity of $\sigma^2_E$. Since we already have the same estimates
available for the total score variance, we can estimate meta-analytic
mean and heterogeneity of true score variance.

$$\hat{\mu}_{\sigma^2_T} = \hat{\mu}_{\sigma^2_X} - \hat{\mu}_{\sigma^2_E} \ \ \ \ \ \ and \ \ \ \ \  \hat{\tau}^2_{\sigma^2_T} = \hat{\tau}^2_{\sigma^2_X} - \hat{\tau}^2_{\sigma^2_E}$$ {#eq-meantauT}

@eq-meantauT provides estimates of meta-analytic mean of true score
variance and heterogeneity. These estimates are largely unbiased, unlike
estimates stemming from using the boot-err to directly run meta-analytic
models on true score variance estimates. Interested readers are referred
to Beinhauer et al. (ref.)

#### Results

@tbl-R1R2 reports the meta-analytic estimates of observed and true score
variances, including their heterogeneity and metrics $R_1$ and $R_2$
across all 12 phenomena. In @tbl-R1R2 we see that for ten out of 12
phenomena, the $R_2$ metric is larger than the $R_1$ metric.
Additionally, we observe that across eight phenomena, $R_2$ is larger
than .96. This implies that of the heterogeneity observed in score
variances, 97% or more could not be explained by differences in random
error introduced by the measuring instrument, but by genuine differences
in the underlying distribution.

```{r, echo = FALSE}
#| label: tbl-R1R2
#| tbl-cap: Re-analysis concerning metrics $R_1$ and $R_2$
knitr::kable(read.csv(here("Tables/Variances_analysis.csv")),
             col.names = c("MASC", 
                           "$\\hat{\\mu}_{\\sigma^2_X}$", 
                           "$\\hat{\\mu}_{\\sigma^2_T}$", 
                           "$\\hat{\\tau}_{\\sigma^2_X}$", 
                           "$\\hat{\\tau}_{\\sigma^2_T}$", 
                           "$R_1$", 
                           "$R_2$"))
```

For the two exceptional phenomena where the attenuation correction
procedure actually led to an increase in ES heterogeneity
(Srull_Behavior_Hostility and Shnabel_Willingness_Reconcile_RPP), the
significance test for the ES heterogeneity was not significant, both
before and after the correction. The results regarding $R_2$ indicate
that the majority of score variance heterogeneity was due to differences
in true score variance $\sigma^2_T$. Since either the heterogeneity was
so small or the sample size so low that the heterogeneity could not be
statistically distinguished from zero, we should be cautious in the
interpretation of a reduction in that estimate. Most likely, the
reduction in heterogeneity is better explained by imprecise estimation
than an actual explanation of unexplained heterogeneity.

Lastly, for two out of the 12 phenomena, the Albarracin_Priming_SAT and
the Shnabel_Willingness_Reconcile_Rev phenomena, we find
$\tau_{\sigma^2_X}$ of 0. Therefore, the $R_2$ metric could not be
computed, as the denominator would have been zero. However, we also did
not identify statistically significant ES heterogeneity in the first
place. The phenomena were assessed with a particularly low number of
replications (8 in both cases). Estimation precision for heterogeneity
in variance components, even more so than for ES, largely depends on the
sample size. Most likely, the low sample size led to implausible
estimates in both true and total score variance heterogeneity and
subsequently to an implausible estimate of $R_2$. Meta-analytic
estimates, especially those concerning score variance components such as
$R_2$, therefore should be interpreted with caution, if the sample size
was particularly low.

Generally, @tbl-tauES implies that for almost all measurements
concerning these phenomena, the heterogeneity in observed score
variances could not be substantially explained by differences in
measurement quality. Only for one phenomenon, where differences in score
variance could be found, $R_2$ was smaller than $R_1^3$. As discussed,
for this phenomenon, the number of administration sites was too low to
accurately estimate heterogeneity in score variance components. For all
remaining phenomena, $R_2$ was larger than $R_1$, implying that an
attenuation correction procedure could not reduce ES heterogeneity. The
results in @tbl-tauES agree with this conclusion, as no phenomena could
be identified where statistically significant ES heterogeneity was
accompanied by a reduction in differences due to the correction.


## Discussion

We have examined meta-analytic heterogeneity in standardized mean
difference. Since score reliability is often discussed as a source for
such heterogeneity, we estimated the heterogeneity before and after
correcting for differences in reliability coefficients using an
attenuation correction procedure. We found that a) in the majority of
cases, the change in ES heterogeneity is small to non-existent. This
implies that, across the data-sets we got to examine, differences in
score reliability do not serve as an appropriate explanation for ES
heterogeneity. We also found that b) in the majority of cases, the
direction of change in ES heterogeneity does not align with assumptions
formulated in W&D and H&S. In almost all cases (every single case, if we
restrict ourself to phenomena where we found statistically significant
differences in ES), the attenuation correction led to increased
estimates of ES heterogeneity. This implies that the claims found in W&D
and H&S stem from a general misunderstanding of how score reliability
can affect ESs.

Using analytical arguments, we derived a broad rule, outlining under
which conditions we can expect an attenuation correction to reduce ES
heterogeneity. In general, score reliability affects ESs by inflating
the score variance, as the (square root of the) score variance is used
to standardize the mean difference. The conditions we found depend on
how both the mean value and the heterogeneity of score variance are
reduced by correcting for imperfect score reliability. We introduced two
novel metrics: $R_1$, describing the fraction of mean true score
variance in the mean total score variance, therefore mirroring the
average score reliability coefficient, and $R_2$, describing the
fraction of true score variance heterogeneity in the total score
variance heterogeneity. We find that, as long as $R_1^3 < R_2$, an
attenuation correction will increase ES heterogeneity.

Since $R_1$ is carrying the power of 3, it implies that the reduction in
mean score variance is weighted more heavily than the reduction in score
variance heterogeneity. This is best described through an example: We
know what an acceptable average score reliability looks like in the area
of Psychology. Researchers largely target a score reliability of at
least .7, preferably .8. An average score reliability of .7 implies that
70% of the variance observed in scores can actually be attributed to
true underlying differences. From our derived rule, we know that, as
long as $R_2$ is larger or equal to $.7^3 = .343$, the attenuation
correction procedure can only increase ES heterogeneity. If we want to
explain ES heterogeneity through score reliability, even though only 30%
of the variance observed in score can be attributed to random
measurement error, at least 65.7%, almost two thirds, of the
heterogeneity in score variance would need to be attributable to
differences in the error score variance. In other words, at most 34.3%
of heterogeneity in score variances may be due to actual differences in
how the latent variable is distributed (in terms of spread) across the
administration sites.

This also means that ES heterogeneity is best reduced, if average score
reliability ($R_1$) is substantially large. In such a scenario, the
$R_2$ is more likely to be low enough for the inequality to no longer
hold. The larger the average score reliability, the more likely it is
that the attenuation correction procedure can actually reduce ES
heterogeneity. However, as the correction would only be rather small in
such cases, it remains to be seen whether the reduction in ES
heterogeneity would be sufficient to claim that differences in score
reliability actually posed a relevant moderator in such a case.

In the examples we re-analysed throughout this text, as well as the
majority of research studies done in the behavioural sciences, no proper
sampling techniques are employed. Most samples where these instruments
are applied come to be through convenience sampling, where students
self-select into studies. Without a valid sampling scheme, we
essentially have have no basis to assume anything about the true score
variance. Most importantly, however, we have no reason to assume that
the true score variance could potentially be homogeneous or stable
across administration sites. Instead, it is very likely that the
convenience sampling procedures produce samples that vary extremely
strongly in how they are put together, leading to large degrees of true
score variance heterogeneity. At the same time, score reliability is a
low-hanging fruit to optimise as most researchers are aware and trained
in assessing score reliability.

Taken together, these facets lead to what we observed throughout this
text: Reliability coefficients are in an acceptable range, leading to a
small, but constant reduction in mean score variance if we apply an
attenuation correction. At the same time, there is substantial variation
in true score variance, implying that most of the score variance
heterogeneity can not be attributed to differences in error score
variance. Since only differences in error score variance can be
corrected for, the attenuation correction has a very small effect on
heterogeneity estimates in score variance. Therefore, as indicated by
our rule, an attenuation correction in these setting can rarely deliver
a reduction in ES heterogeneity. This also means that, in such settings,
score reliability does not explain any ES heterogeneity. If anything, it
masks it to a small degree.

If we, in the space of Psychology, keep making use of convenience
samples simply because they are so convenient, we can not expect
anything to change. Most likely, ES heterogeneity will remain a
substantial factor hampering the interpretability of original and
replication studies. We could learn more about standardized effect sizes
and why our phenomena vary across settings, populations or continents by
not following the current status quo of convenience samples. Until this
status quo changes, however, we will keep wasting valuable resources by
attempts of discussing ES heterogeneity or assessing uninformative
differences in score reliability.

These results fit in with work recently published/preprinted by
Olsson-Collentine et al. (2023). In a large simulation scheme, they
demonstrate that differences in score reliability across administrations
typically deflate heterogeneity in uncorrected correlations. Only as the
true heterogeneity in correlations grows larger, score reliability
differences actually inflate heterogeneity as claimed in W&D. While
correlations and standardized effect sizes are not identical, the way
score (un)reliability affects these parameters is highly similar.

Heterogeneity in ES even in direct replications, where experimental
factors are held as constant as possible, is already substantial
(Renkewitz, Fünderich, & Beinhauer, 2024). However, so far we have
demonstrated that in several scenarios, differences in score reliability
did not serve as appropriate moderators or explanations of ES
heterogeneity. Across most phenomena, we failed to reduce ES
heterogeneity. If anything, we even increased ES heterogeneity, albeit
to a very small degree. Generally, concerning the multi-site replication
projects or collaborative research projects, accounting for differences
in score reliability did not help us gain a better understanding of the
phenomena.

The empirical arguments presented are based on a rather small number of
non-representative data-sets. Based on the combination of analytical and
empirical arguments, we are convinced that these differences in score
reliability across replications in the current state of Psychology and
the behavioural sciences, do not serve as appropriate explanations of ES
heterogeneity. However, whether these results actually generalize beyond
these data-sets remains to be seen. Unfortunately, the behavioural
sciences are in dire need of open-data that resemble multi-site
replications. As the majority of results discussed over the last years
(in e.g. ManyLabs or Registered Replication Reports) employ
single-indicator scales as dependent measures, score reliability can not
be easily estimated in order to replicate our analyses.

The analytical arguments presented following Equation 17 rested on the
use of the delta-method, as Equation 17 contained parameters of the
score standard deviations’ distributions, while the $R_1$ and $R_2$
metrics were designed to discuss score variances. The delta-method rests
on a number of assumptions: (1) approximations derived from the
delta-method require for the transformed variable to follow a marginal
normal distribution (ref). Since the variable under discussion here is
the standard deviation, which is bounded to be larger than zero and has
a non-normal sampling distribution (ref), this assumption is most likely
violated; (2) the derivative of the transformation function needs to be
available. As the transformation function is essentially just the square
root, this assumption should hold up; (3) as the delta-method
approximates the parameters of the transformed variable using the
first-order Taylor-expansion, this implies that higher-order terms are
required to be negligible. In practice, this implies that the variance
of the untransformed variable needs to be small. However, as this entire
procedure discusses the influence of variance in standard deviations on
ES heterogeneity, introduced by differences in score reliability, this
assumption is probably violated as well. The violation of these
assumptions likely introduce a bias in estimates derived from these
approximations (ref). However, the inequality derived with use of the
delta-method is not used to actually derive any estimates, but to
discuss how an attenuation correction procedure can affect ES
heterogeneity. Therefore, we believe that the violation of these
assumptions is defensible in this case, as the analytical arguments
derived from the inequality should still hold up, nevertheless. \[add to
appendix!\] Additionally, we used the empirical examples available to
assess in how far estimates derived using the delta-method
approximations diverge from estimates derived without it. The
differences identified are rather miniscule at the third decimal,
reinforcing our belief that these assumption violations are defensible.

Similarly, all Equations following Equation 10 rest on the assumption
that mean difference ($MD$) and true or total score variance
($\sigma^2_T$ and $\sigma^2_X$) are independent. While this assumption
is in line with assumptions underlying CTT, it is by no means guaranteed
that it holds in these data-sets. If this assumption was violated,
whether the relationship was positive or negative, indicated by a
positive or negative covariance in Equation 10, would most likely have
an impact on how strongly ES heterogeneity is affected by the
attenuation correction procedure, possibly even the direction of how it
is affected. However, currently we have no reason to assume that there
is a systematic relationship between mean difference and standard
deviation. As this relationship would also affect the distribution of ES
like Cohen’s d itself, strong violations of this assumption would go far
beyond invalidating the claims we derived in Equation 16. Instead,
assumptions of the meta-analytic tests concerning the size ES and the
presence of ES heterogeneity would be violated, making any research on
change in ES heterogeneity due to attenuation correction obsolete.

The work done by Hunter and Schmidt (2014) was crucial in informing and
guiding methodology development in the field of meta-analyses.
Similarly, Wiernik and Dahlke (2020) raise a number of important points
that have been neglected in the application of meta-analytic research
over the last decades. We generally agree with their ideas that
underappreciated (differences in un-)reliability can introduce
substantial biases in meta-analytic estimates and tests in Psychology.
However, based on our findings, we believe that the issues created by
ignoring random sampling is much more dire and warrants immediate
attention. Only once we can be sure that the variance in true scores or
latent factors is actually meaningful for some population, we can turn
towards imprecise measurements, expecting correction procedures to
explain unwarranted heterogeneity. In the current state of Psychology,
meta-analysts should not expect that correcting ES for score reliability
will actually explain heterogeneity identified.
